{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import threading\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from   numpy import float32\n",
    "from   tensorflow.keras import layers,Model\n",
    "import pickle5 as pickle\n",
    "from   tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from   tensorflow.keras.activations import tanh\n",
    "from   tensorflow.keras.activations import softmax\n",
    "\n",
    "from Config import Config\n",
    "import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self,cfg,tailModel):\n",
    "        self.cfg = cfg\n",
    "        self.tailModel = tailModel\n",
    "        self.s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "        self.accept_connections()\n",
    "    \n",
    "    def accept_connections(self):\n",
    "        ip = '' \n",
    "        port = self.cfg.server_port\n",
    "\n",
    "        print('Running on IP: '+ip)\n",
    "        print('Running on port: '+str(port))\n",
    "\n",
    "        self.s.bind((ip,port))\n",
    "        self.s.listen(100)\n",
    "\n",
    "        while 1:\n",
    "            try:\n",
    "                c, addr = self.s.accept()\n",
    "            except KeyboardInterrupt as e:\n",
    "                print(\"ctrl+c,Exiting gracefully\")\n",
    "                self.s.shutdown(socket.SHUT_RDWR)\n",
    "                self.s.close()\n",
    "                exit(0)\n",
    "            # print(c)\n",
    "\n",
    "            threading.Thread(target=self.handle_client,args=(c,addr,)).start()\n",
    "\n",
    "    def handle_client(self,c,addr):\n",
    "        print(addr)\n",
    "        Logger.debug_print(\"handle_client:Entry\")\n",
    "        received_data = c.recv(1024).decode()\n",
    "        Logger.debug_print(\"handle_client:received_data=\"+received_data)\n",
    "        obj = json.loads(received_data)\n",
    "        Logger.debug_print(obj)\n",
    "        tensor_shape = obj['data_shape']\n",
    "        Logger.debug_print(\"handle_client:sending OK\")\n",
    "        c.send(\"OK\".encode())\n",
    "\n",
    "        max_data_to_be_received = obj['data_size']\n",
    "        total_data = 0\n",
    "        msg = bytearray()\n",
    "        while 1:\n",
    "            # print(\"handle_client:calling recv total_data=%d data_size=%d\" % (total_data, max_data_to_be_received))\n",
    "            if(total_data >= max_data_to_be_received):\n",
    "                Logger.debug_print(\"handle_client:received all data\")\n",
    "                break\n",
    "            data = c.recv(1024)\n",
    "            # print(type(data))\n",
    "            msg.extend(data)\n",
    "            if not data:\n",
    "                Logger.debug_print(\"handle_client:while break\")\n",
    "                break\n",
    "            total_data += len(data)\n",
    "        \n",
    "        Logger.debug_print('total size of msg=%d' % (len(msg)))\n",
    "        \n",
    "        generated_np_array = np.frombuffer(msg, dtype=float32)\n",
    "        generated_image_np_array = generated_np_array.reshape(tensor_shape)\n",
    "        generate_image_tensor = tf.convert_to_tensor(generated_image_np_array, dtype=tf.float32)\n",
    "        result, attention_plot,pred_test  = tailModel.evaluate(generate_image_tensor)\n",
    "        pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "        Logger.debug_print(\"handle_client:sending pred_caption\" + pred_caption)\n",
    "        c.send(pred_caption.encode())\n",
    "        # candidate = pred_caption.split()\n",
    "        Logger.debug_print ('Pred:' + pred_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # build your Dense layer with relu activation\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation='relu')\n",
    "        \n",
    "    def call(self, features):\n",
    "        # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "        features = self.dense(features)\n",
    "        return features    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.units=units\n",
    "        # build your Dense layer\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        # build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # build your final Dense layer with unit 1\n",
    "        # self.V = tf.keras.layers.Dense(1, activation='softmax')\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features shape: (batch_size, 8*8, embedding_dim)\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        score = tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        score = self.V(score)\n",
    "        attention_weights = softmax(score, axis=1)\n",
    "\n",
    "        # shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        # reduce the shape to (batch_size, embedding_dim)\n",
    "        # context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.reduce_mean(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size, cfg):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.units = units\n",
    "        # iniitalise your Attention model with units\n",
    "        self.attention = Attention_model(self.units)\n",
    "        # build your Embedding layer\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, self.cfg.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        # build your Dense layer\n",
    "        self.d1 = tf.keras.layers.Dense(self.units)\n",
    "        # build your Dense layer\n",
    "        self.d2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        #create your context vector & attention weights from attention model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed = self.embed(x)\n",
    "        # Concatenate your input with the context vector from attention layer. \n",
    "        # Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)\n",
    "        # Extract the output & hidden state from GRU layer. \n",
    "        # Output shape : (batch_size, max_length, hidden_size)\n",
    "        output, state = self.gru(embed)\n",
    "        output = self.d1(output)\n",
    "        # shape : (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) \n",
    "        # shape : (batch_size * max_length, vocab_size)\n",
    "        output = self.d2(output)\n",
    "\n",
    "        return output, state, attention_weights\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TailModel:\n",
    "    def __init__(self,cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "        with open(self.cfg.saved_model_path + '/tokenizer.pickle', 'rb') as handle:\n",
    "            self.tokenizer = pickle.load(handle)\n",
    "        \n",
    "        with open(self.cfg.saved_model_path + '/image_features_extract_model.json', 'r') as json_file:\n",
    "            json_savedModel= json_file.read()\n",
    "        self.image_features_extract_model = tf.keras.models.model_from_json(json_savedModel)\n",
    "        self.image_features_extract_model.load_weights(self.cfg.saved_model_path + '/image_features_extract_model.h5')\n",
    "        vocab_size = cfg.max_tokenized_words + 1\n",
    "\n",
    "        s = tf.zeros([32, 64, 2048], tf.int32)\n",
    "\n",
    "        self.encoder=Encoder(self.cfg.embedding_dim)\n",
    "\n",
    "        self.decoder=Decoder(self.cfg.embedding_dim, self.cfg.units, vocab_size,cfg)\n",
    "\n",
    "        features = self.encoder(s)\n",
    "\n",
    "        hidden = self.decoder.init_state(batch_size=self.cfg.batch_size)\n",
    "        dec_input = tf.expand_dims([self.tokenizer.word_index['<start>']] * self.cfg.batch_size, 1)\n",
    "\n",
    "        predictions, hidden_out, attention_weights= self.decoder(dec_input, features, hidden)\n",
    "\n",
    "        self.decoder.load_weights(self.cfg.saved_model_path + \"/decoder.h5\")\n",
    "        self.encoder.load_weights(self.cfg.saved_model_path + \"/encoder.h5\")\n",
    "\n",
    "    def evaluate(self,image):\n",
    "        attention_features_shape = 64\n",
    "        attention_plot = np.zeros((self.cfg.MAX_SEQ_LENGTH, attention_features_shape))\n",
    "\n",
    "        hidden = self.decoder.init_state(batch_size=1)\n",
    "\n",
    "        # process the input image to desired format before extracting features\n",
    "        temp_input = tf.expand_dims(image, 0) \n",
    "        # Extract features using our feature extraction model\n",
    "        img_tensor_val = self.extract_image_features(temp_input)\n",
    "        # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "        # extract the features by passing the input to encoder\n",
    "        features = self.encoder(img_tensor_val)\n",
    "\n",
    "        dec_input = tf.expand_dims([self.tokenizer.word_index['<start>']], 0)\n",
    "        result = []\n",
    "\n",
    "        for i in range(self.cfg.MAX_SEQ_LENGTH):\n",
    "            # get the output from decoder\n",
    "            predictions, hidden, attention_weights = self.decoder(dec_input, features, hidden)\n",
    "\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "            #extract the predicted id(embedded value) which carries the max value\n",
    "            predicted_id = tf.argmax(tf.transpose(predictions))\n",
    "            predicted_id = predicted_id.numpy()[0]\n",
    "            # map the id to the word from tokenizer and append the value to the result list\n",
    "            result.append(self.tokenizer.index_word[predicted_id])\n",
    "\n",
    "            if self.tokenizer.index_word[predicted_id] == '<end>':\n",
    "                return result, attention_plot,predictions\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        attention_plot = attention_plot[:len(result), :]\n",
    "        return result, attention_plot,predictions\n",
    "\n",
    "    def extract_image_features(self, sample_img_batch):\n",
    "        features = self.image_features_extract_model(sample_img_batch)\n",
    "        features = tf.reshape(features, [sample_img_batch.shape[0],8*8, 2048])\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "tailModel = TailModel(cfg)\n",
    "server = Server(cfg, tailModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}