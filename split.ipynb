{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install --upgrade git+https://github.com/EmGarr/kerod.git"
   ],
   "outputs": [],
   "metadata": {
    "id": "1jVKNjhdLFUQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import  functools\n",
    "import  tensorflow as tf\n",
    "import  tensorflow_datasets as tfds\n",
    "from    tensorflow.keras.utils import to_categorical\n",
    "import  matplotlib.pyplot as plt\n",
    "from    tensorflow.keras import layers\n",
    "from    tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "import pandas as pd\n",
    "from    tqdm import tqdm\n",
    "import  time\n",
    "from    sklearn.metrics import accuracy_score\n",
    "\n",
    "from common.config import Config\n",
    "from common.logger import Logger\n",
    "from common.communication import Client\n",
    "from common.communication import Server\n",
    "from common.helper import ImagesInfo \n",
    "from common.timekeeper import TimeKeeper\n",
    "from common.helper import read_image, filt_text, get_predictions\n",
    "from CaptionModel import CaptionModel\n",
    "from common.helper import read_image, filt_text, get_predictions,process_predictions\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_dir='/home/suphale/coco'\n",
    "N_LABELS = 80\n",
    "N_EPOCHS = 1\n",
    "TRAIN_MODE = False\n",
    "split_train = \"train[:1%]\"\n",
    "split_val = \"validation[:1%]\"\n",
    "h_image_height = 299\n",
    "h_image_width = 299\n",
    "\n",
    "SPLIT_LAYER = 4\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tk = TimeKeeper()\n",
    "cfg = Config()\n",
    "client = Client(cfg)\n",
    "imagesInfo = ImagesInfo(cfg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from tensorflow import keras  # or import keras for standalone version\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "org_model = tf.keras.models.load_model(cfg.temp_path + '/extractor_model')\n",
    "# org_model = tf.keras.models.load_model(cfg.saved_model_path + '/model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model_config = org_model.get_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "max_layer_index = len(model_config['layers']) - 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "head_model_config = {}\n",
    "head_model_config['name'] = 'head_model'\n",
    "head_model_config['layers'] = []\n",
    "head_model_config['input_layers'] = [[model_config['layers'][0]['name'],0,0]]\n",
    "head_model_config['output_layers'] = [[model_config['layers'][SPLIT_LAYER-1]['name'],0,0]]\n",
    "\n",
    "for index in range(SPLIT_LAYER):\n",
    "    print(\"%d %s\" % (index, model_config['layers'][index]['name']) )\n",
    "    head_model_config['layers'].append(model_config['layers'][index])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 input_1\n",
      "1 conv2d\n",
      "2 batch_normalization\n",
      "3 activation\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import copy\n",
    "\n",
    "tail_model_config = copy.deepcopy(model_config)\n",
    "tail_model_config['name'] = 'tail_model'\n",
    "tail_model_config['input_layers'] = [[model_config['layers'][SPLIT_LAYER]['name'],0,0]]\n",
    "# tail_model_config['output_layers'] = [[model_config['layers'][max_layer_index]['name'],0,0]]\n",
    "tail_model_config['output_layers'] = model_config['output_layers']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print(tail_model_config['output_layers'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['mixed10', 0, 0], ['dense_1', 0, 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "new_input_layer = {\n",
    "                      'name': 'new_input',\n",
    "                      'class_name': 'InputLayer',\n",
    "                      'config': {\n",
    "                          'batch_input_shape': tuple(org_model.layers[SPLIT_LAYER-1].output.shape),\n",
    "                          'dtype': 'float32',\n",
    "                          'sparse': False,\n",
    "                          'name': 'new_input'\n",
    "                      },\n",
    "                      'inbound_nodes': []\n",
    "                  }\n",
    "tail_model_config['layers'][0] = new_input_layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "for index in range(1,SPLIT_LAYER):\n",
    "    print(\"%d %s\" % (index, tail_model_config['layers'][1]['name']) )\n",
    "    tail_model_config['layers'].pop(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 conv2d\n",
      "2 batch_normalization\n",
      "3 activation\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "tail_model_config['layers'][1]['inbound_nodes'] = [[['new_input', 0, 0, {}]]]\n",
    "tail_model_config['input_layers'] = [['new_input', 0, 0]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import pprint\n",
    "with open(cfg.temp_path + '/model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(model_config))\n",
    "    print(model_config,file=fh)\n",
    "with open(cfg.temp_path + '/head_model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(new_head_model_config))\n",
    "    print(head_model_config,file=fh)\n",
    "with open(cfg.temp_path + '/tail_model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(new_head_model_config))\n",
    "    print(tail_model_config,file=fh)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "head_model = org_model.__class__.from_config(head_model_config, custom_objects={})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "tail_model = org_model.__class__.from_config(tail_model_config, custom_objects={})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "with open(cfg.temp_path + '/org_model.txt','w') as fh:\n",
    "    org_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n",
    "\n",
    "with open(cfg.temp_path + '/head_model.txt','w') as fh:\n",
    "    head_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n",
    "\n",
    "with open(cfg.temp_path + '/tail_model.txt','w') as fh:\n",
    "    tail_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for index, layer in enumerate(org_model.layers[:SPLIT_LAYER]):\n",
    "    # print(\"[%d] %s %s\" % (index, layer.name, str(np.shape(weight))))\n",
    "    weight = layer.get_weights()\n",
    "    new_head_model_layer = head_model.layers[index]\n",
    "    new_head_model_layer.set_weights(weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import numpy as np\n",
    "for index, layer in enumerate(org_model.layers[SPLIT_LAYER:max_layer_index+1]):\n",
    "    weight = layer.get_weights()\n",
    "    # print(\"org_model [%d] %s %s\" % (index, layer.name, str(tf.shape(weight))))\n",
    "    tail_model_layer = tail_model.layers[index+1]\n",
    "    tail_model_layer_weight = tail_model_layer.get_weights()\n",
    "    # print(\"tail_model [%d] %s %s\" % (index, tail_model_layer.name, str(tf.shape(tail_model_layer_weight))))\n",
    "    tail_model_layer.set_weights(weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "head_model.save(cfg.temp_path + '/head_model')\n",
    "tail_model.save(cfg.temp_path + '/tail_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: /home/suphale/WorkSpace/temp/head_model/assets\n",
      "INFO:tensorflow:Assets written to: /home/suphale/WorkSpace/temp/tail_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class BoxField:\n",
    "    BOXES = 'bbox'\n",
    "    KEYPOINTS = 'keypoints'\n",
    "    LABELS = 'label'\n",
    "    MASKS = 'masks'\n",
    "    NUM_BOXES = 'num_boxes'\n",
    "    SCORES = 'scores'\n",
    "    WEIGHTS = 'weights'\n",
    "\n",
    "class DatasetField:\n",
    "    IMAGES = 'images'\n",
    "    IMAGES_INFO = 'images_information'\n",
    "    IMAGES_PMASK = 'images_padding_mask'\n",
    "\n",
    "def my_preprocess(inputs):\n",
    "    image = inputs['image']\n",
    "    image = tf.image.resize(image, (h_image_height, h_image_width))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "\n",
    "    targets = inputs['objects']\n",
    "    img_path = inputs['image/filename']\n",
    "\n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    inputs = {DatasetField.IMAGES: image, DatasetField.IMAGES_INFO: image_information}\n",
    "\n",
    "    # ground_truths = {\n",
    "    #     BoxField.BOXES: targets[BoxField.BOXES] * tf.tile(image_information[tf.newaxis], [1, 2]),\n",
    "    #     BoxField.LABELS: tf.cast(targets[BoxField.LABELS], tf.int32),\n",
    "    #     BoxField.NUM_BOXES: tf.shape(targets[BoxField.LABELS]),\n",
    "    #     BoxField.WEIGHTS: tf.fill(tf.shape(targets[BoxField.LABELS]), 1.0)\n",
    "    # }\n",
    "    ground_truths = tf.cast(targets[BoxField.LABELS], tf.int32)\n",
    "    # ground_truths = tf.one_hot(ground_truths, depth=N_LABELS, dtype=tf.int32)\n",
    "    # ground_truths = tf.reduce_sum(ground_truths, 0)\n",
    "    # ground_truths = tf.greater( ground_truths, tf.constant( 0 ) )    \n",
    "    # ground_truths = tf.where (ground_truths, 1, 0) \n",
    "    return image, ground_truths, img_path\n",
    "\n",
    "def expand_dims_for_single_batch(image, ground_truths, img_path):\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    ground_truths = tf.expand_dims(ground_truths, axis=0)\n",
    "    return image, ground_truths, img_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "ds_val, ds_info = tfds.load(name=\"coco/2017\", split=split_val, data_dir=data_dir, shuffle_files=False, download=False, with_info=True)\n",
    "ds_val = ds_val.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f6aa79bdd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f6aa79bdd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function my_preprocess at 0x7f6aa79bdd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from   nltk.translate.bleu_score import sentence_bleu\n",
    "def process_caption_predictions(caption_tensor, img_path):\n",
    "    pred_caption=' '.join(caption_tensor).rsplit(' ', 1)[0]\n",
    "    real_appn = []\n",
    "    real_caption_list = imagesInfo.annotations_dict[img_path]\n",
    "    for real_caption in real_caption_list:\n",
    "        real_caption=filt_text(real_caption)\n",
    "        real_appn.append(real_caption.split())\n",
    "    reference = real_appn\n",
    "    candidate = pred_caption.split()\n",
    "    score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "    return score,real_caption,pred_caption"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "Test = True\n",
    "captionModel = CaptionModel()\n",
    "if (Test == True):\n",
    "    ds_val = ds_val.take(1)\n",
    "    for sample_img_batch, ground_truth, img_path in tqdm(ds_val):\n",
    "        # count += 1\n",
    "\n",
    "        tensor_shape = len(ground_truth.get_shape().as_list())\n",
    "        if(tensor_shape > 1):\n",
    "            ground_truth = tf.squeeze(ground_truth,[0])\n",
    "        ground_truth = list(set(ground_truth.numpy()))\n",
    "\n",
    "        img_path = img_path.numpy().decode()\n",
    "        print(img_path)\n",
    "        h = head_model(sample_img_batch)\n",
    "        features, result = tail_model(h)\n",
    "        # features, result = model(sample_img_batch)\n",
    "        predictions, predictions_prob = get_predictions(cfg, result)\n",
    "        accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(cfg, imagesInfo, ground_truth,predictions, predictions_prob)\n",
    "        print(predictions_str)\n",
    "\n",
    "        features = tf.reshape(features, [sample_img_batch.shape[0],8*8, 2048])\n",
    "        caption_tensor = captionModel.evaluate(features)\n",
    "\n",
    "        print(type(caption_tensor))\n",
    "        score,real_caption,pred_caption = process_caption_predictions(caption_tensor, img_path)\n",
    "\n",
    "        print(\"BLEU: %.2f\" % (score))\n",
    "        print ('Real:', real_caption)\n",
    "        print ('Pred:', pred_caption)    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "000000531036.jpg\n",
      "person(69.00) car(58.00) bus(89.00) truck(48.00) \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n",
      "BLEU: 0.60\n",
      "Real: a double decker bus is stopped along a curb\n",
      "Pred: a red double decker bus is driving down the street\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coco_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py373",
   "language": "python",
   "name": "py373"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}