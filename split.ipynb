{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install --upgrade git+https://github.com/EmGarr/kerod.git"
   ],
   "outputs": [],
   "metadata": {
    "id": "1jVKNjhdLFUQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import  functools\n",
    "import  tensorflow as tf\n",
    "import  tensorflow_datasets as tfds\n",
    "from    tensorflow.keras.utils import to_categorical\n",
    "import  matplotlib.pyplot as plt\n",
    "from    tensorflow.keras import layers\n",
    "from    tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "import pandas as pd\n",
    "from    tqdm import tqdm\n",
    "import  time\n",
    "from    sklearn.metrics import accuracy_score\n",
    "\n",
    "from common.config import Config\n",
    "from common.logger import Logger\n",
    "from common.communication import Client\n",
    "from common.communication import Server\n",
    "from common.helper import ImagesInfo \n",
    "from common.timekeeper import TimeKeeper\n",
    "from common.helper import read_image, filt_text, get_predictions\n",
    "from CaptionModel import CaptionModel\n",
    "from common.helper import read_image, filt_text, get_predictions,process_predictions\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_dir='/home/suphale/coco'\n",
    "N_LABELS = 80\n",
    "N_EPOCHS = 1\n",
    "TRAIN_MODE = False\n",
    "split_train = \"train[:1%]\"\n",
    "split_val = \"validation[:1%]\"\n",
    "h_image_height = 299\n",
    "h_image_width = 299\n",
    "\n",
    "SPLIT_LAYER = 248 + 1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tk = TimeKeeper()\n",
    "cfg = Config()\n",
    "client = Client(cfg)\n",
    "imagesInfo = ImagesInfo(cfg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from tensorflow import keras  # or import keras for standalone version\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "org_model = tf.keras.models.load_model(cfg.temp_path + '/model')\n",
    "# org_model = tf.keras.models.load_model(cfg.saved_model_path + '/model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model_config = org_model.get_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "max_layer_index = len(model_config['layers']) - 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "head_model_config = {}\n",
    "head_model_config['name'] = 'head_model'\n",
    "head_model_config['layers'] = []\n",
    "head_model_config['input_layers'] = [[model_config['layers'][0]['name'],0,0]]\n",
    "head_model_config['output_layers'] = [[model_config['layers'][SPLIT_LAYER-1]['name'],0,0]]\n",
    "\n",
    "for index in range(SPLIT_LAYER):\n",
    "    print(\"%d %s\" % (index, model_config['layers'][index]['name']) )\n",
    "    head_model_config['layers'].append(model_config['layers'][index])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 input_1\n",
      "1 conv2d\n",
      "2 batch_normalization\n",
      "3 activation\n",
      "4 conv2d_1\n",
      "5 batch_normalization_1\n",
      "6 activation_1\n",
      "7 conv2d_2\n",
      "8 batch_normalization_2\n",
      "9 activation_2\n",
      "10 max_pooling2d\n",
      "11 conv2d_3\n",
      "12 batch_normalization_3\n",
      "13 activation_3\n",
      "14 conv2d_4\n",
      "15 batch_normalization_4\n",
      "16 activation_4\n",
      "17 max_pooling2d_1\n",
      "18 conv2d_8\n",
      "19 batch_normalization_8\n",
      "20 activation_8\n",
      "21 conv2d_6\n",
      "22 conv2d_9\n",
      "23 batch_normalization_6\n",
      "24 batch_normalization_9\n",
      "25 activation_6\n",
      "26 activation_9\n",
      "27 average_pooling2d\n",
      "28 conv2d_5\n",
      "29 conv2d_7\n",
      "30 conv2d_10\n",
      "31 conv2d_11\n",
      "32 batch_normalization_5\n",
      "33 batch_normalization_7\n",
      "34 batch_normalization_10\n",
      "35 batch_normalization_11\n",
      "36 activation_5\n",
      "37 activation_7\n",
      "38 activation_10\n",
      "39 activation_11\n",
      "40 mixed0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Last layer of the head model\n",
    "last_head_model_layer = head_model_config['layers'][SPLIT_LAYER-1]['name']\n",
    "print(last_head_model_layer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mixed0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# First layer of the tail model\n",
    "[[model_config['layers'][SPLIT_LAYER]['name'],0,0]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['conv2d_15', 0, 0]]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import copy\n",
    "\n",
    "tail_model_config = copy.deepcopy(model_config)\n",
    "tail_model_config['name'] = 'tail_model'\n",
    "tail_model_config['input_layers'] = [[model_config['layers'][SPLIT_LAYER]['name'],0,0]]\n",
    "# tail_model_config['output_layers'] = [[model_config['layers'][max_layer_index]['name'],0,0]]\n",
    "tail_model_config['output_layers'] = model_config['output_layers']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(tail_model_config['input_layers'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['conv2d_15', 0, 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(tail_model_config['output_layers'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['mixed10', 0, 0], ['dense_1', 0, 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "new_input_layer = {\n",
    "                      'name': 'new_input',\n",
    "                      'class_name': 'InputLayer',\n",
    "                      'config': {\n",
    "                          'batch_input_shape': tuple(org_model.layers[SPLIT_LAYER-1].output.shape),\n",
    "                          'dtype': 'float32',\n",
    "                          'sparse': False,\n",
    "                          'name': 'new_input'\n",
    "                      },\n",
    "                      'inbound_nodes': []\n",
    "                  }\n",
    "tail_model_config['layers'][0] = new_input_layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for index in range(1,SPLIT_LAYER):\n",
    "    print(\"%d %s\" % (index, tail_model_config['layers'][1]['name']) )\n",
    "    tail_model_config['layers'].pop(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 conv2d\n",
      "2 batch_normalization\n",
      "3 activation\n",
      "4 conv2d_1\n",
      "5 batch_normalization_1\n",
      "6 activation_1\n",
      "7 conv2d_2\n",
      "8 batch_normalization_2\n",
      "9 activation_2\n",
      "10 max_pooling2d\n",
      "11 conv2d_3\n",
      "12 batch_normalization_3\n",
      "13 activation_3\n",
      "14 conv2d_4\n",
      "15 batch_normalization_4\n",
      "16 activation_4\n",
      "17 max_pooling2d_1\n",
      "18 conv2d_8\n",
      "19 batch_normalization_8\n",
      "20 activation_8\n",
      "21 conv2d_6\n",
      "22 conv2d_9\n",
      "23 batch_normalization_6\n",
      "24 batch_normalization_9\n",
      "25 activation_6\n",
      "26 activation_9\n",
      "27 average_pooling2d\n",
      "28 conv2d_5\n",
      "29 conv2d_7\n",
      "30 conv2d_10\n",
      "31 conv2d_11\n",
      "32 batch_normalization_5\n",
      "33 batch_normalization_7\n",
      "34 batch_normalization_10\n",
      "35 batch_normalization_11\n",
      "36 activation_5\n",
      "37 activation_7\n",
      "38 activation_10\n",
      "39 activation_11\n",
      "40 mixed0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find if any layer in the tail model takes the last layer of head model as input\n",
    "# substitute it with the input layer\n",
    "# Ideally we should check if any tail model layer refers to any head model layer ToDo\n",
    "for index, layer in enumerate(tail_model_config['layers']):\n",
    "    if (np.shape(layer['inbound_nodes'])[0] > 0):\n",
    "        dim_1 = len(layer['inbound_nodes'][0])\n",
    "        if(dim_1 >= 1):\n",
    "            for i in range(dim_1):\n",
    "                in_layer = layer['inbound_nodes'][0][i][0]\n",
    "                if(in_layer == last_head_model_layer):\n",
    "                    print(str(index) + \"    \" + layer['name'] + \" \" + in_layer )\n",
    "                    # print(tail_model_config['layers'][index]['inbound_nodes'][0][i])\n",
    "                    tail_model_config['layers'][index]['inbound_nodes'][0][i] = [[['new_input', 0, 0, {}]]]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1    conv2d_15 mixed0\n",
      "4    conv2d_13 mixed0\n",
      "10    average_pooling2d_1 mixed0\n",
      "11    conv2d_12 mixed0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# tail_model_config['layers'][1]['inbound_nodes'] = [[['new_input', 0, 0, {}]]]\n",
    "tail_model_config['input_layers'] = [['new_input', 0, 0]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import pprint\n",
    "with open(cfg.temp_path + '/model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(model_config))\n",
    "    print(model_config,file=fh)\n",
    "with open(cfg.temp_path + '/head_model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(new_head_model_config))\n",
    "    print(head_model_config,file=fh)\n",
    "with open(cfg.temp_path + '/tail_model_config.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    # fh.write(str(new_head_model_config))\n",
    "    print(tail_model_config,file=fh)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "head_model = org_model.__class__.from_config(head_model_config, custom_objects={})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "tail_model = org_model.__class__.from_config(tail_model_config, custom_objects={})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "with open(cfg.temp_path + '/org_model.txt','w') as fh:\n",
    "    org_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n",
    "\n",
    "with open(cfg.temp_path + '/head_model.txt','w') as fh:\n",
    "    head_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n",
    "\n",
    "with open(cfg.temp_path + '/tail_model.txt','w') as fh:\n",
    "    tail_model.summary(print_fn=lambda x: fh.write(x + '\\n'), line_length=150)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for index, layer in enumerate(org_model.layers[:SPLIT_LAYER]):\n",
    "    # print(\"[%d] %s %s\" % (index, layer.name, str(np.shape(weight))))\n",
    "    weight = layer.get_weights()\n",
    "    new_head_model_layer = head_model.layers[index]\n",
    "    new_head_model_layer.set_weights(weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import numpy as np\n",
    "for index, layer in enumerate(org_model.layers[SPLIT_LAYER:max_layer_index+1]):\n",
    "    weight = layer.get_weights()\n",
    "    # print(\"org_model [%d] %s %s\" % (index, layer.name, str(tf.shape(weight))))\n",
    "    tail_model_layer = tail_model.layers[index+1]\n",
    "    tail_model_layer_weight = tail_model_layer.get_weights()\n",
    "    # print(\"tail_model [%d] %s %s\" % (index, tail_model_layer.name, str(tf.shape(tail_model_layer_weight))))\n",
    "    tail_model_layer.set_weights(weight)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "head_model.save(cfg.temp_path + '/head_model_'+str(SPLIT_LAYER))\n",
    "tail_model.save(cfg.temp_path + '/tail_model_'+str(SPLIT_LAYER))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: /home/suphale/WorkSpace/temp/head_model/assets\n",
      "INFO:tensorflow:Assets written to: /home/suphale/WorkSpace/temp/tail_model/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class BoxField:\n",
    "    BOXES = 'bbox'\n",
    "    KEYPOINTS = 'keypoints'\n",
    "    LABELS = 'label'\n",
    "    MASKS = 'masks'\n",
    "    NUM_BOXES = 'num_boxes'\n",
    "    SCORES = 'scores'\n",
    "    WEIGHTS = 'weights'\n",
    "\n",
    "class DatasetField:\n",
    "    IMAGES = 'images'\n",
    "    IMAGES_INFO = 'images_information'\n",
    "    IMAGES_PMASK = 'images_padding_mask'\n",
    "\n",
    "def my_preprocess(inputs):\n",
    "    image = inputs['image']\n",
    "    image = tf.image.resize(image, (h_image_height, h_image_width))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "\n",
    "    targets = inputs['objects']\n",
    "    img_path = inputs['image/filename']\n",
    "\n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    inputs = {DatasetField.IMAGES: image, DatasetField.IMAGES_INFO: image_information}\n",
    "\n",
    "    # ground_truths = {\n",
    "    #     BoxField.BOXES: targets[BoxField.BOXES] * tf.tile(image_information[tf.newaxis], [1, 2]),\n",
    "    #     BoxField.LABELS: tf.cast(targets[BoxField.LABELS], tf.int32),\n",
    "    #     BoxField.NUM_BOXES: tf.shape(targets[BoxField.LABELS]),\n",
    "    #     BoxField.WEIGHTS: tf.fill(tf.shape(targets[BoxField.LABELS]), 1.0)\n",
    "    # }\n",
    "    ground_truths = tf.cast(targets[BoxField.LABELS], tf.int32)\n",
    "    # ground_truths = tf.one_hot(ground_truths, depth=N_LABELS, dtype=tf.int32)\n",
    "    # ground_truths = tf.reduce_sum(ground_truths, 0)\n",
    "    # ground_truths = tf.greater( ground_truths, tf.constant( 0 ) )    \n",
    "    # ground_truths = tf.where (ground_truths, 1, 0) \n",
    "    return image, ground_truths, img_path\n",
    "\n",
    "def expand_dims_for_single_batch(image, ground_truths, img_path):\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    ground_truths = tf.expand_dims(ground_truths, axis=0)\n",
    "    return image, ground_truths, img_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "ds_val, ds_info = tfds.load(name=\"coco/2017\", split=split_val, data_dir=data_dir, shuffle_files=False, download=False, with_info=True)\n",
    "ds_val = ds_val.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7fb568cf7cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7fb568cf7cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function my_preprocess at 0x7fb568cf7cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from   nltk.translate.bleu_score import sentence_bleu\n",
    "def process_caption_predictions(caption_tensor, img_path):\n",
    "    pred_caption=' '.join(caption_tensor).rsplit(' ', 1)[0]\n",
    "    real_appn = []\n",
    "    real_caption_list = imagesInfo.annotations_dict[img_path]\n",
    "    for real_caption in real_caption_list:\n",
    "        real_caption=filt_text(real_caption)\n",
    "        real_appn.append(real_caption.split())\n",
    "    reference = real_appn\n",
    "    candidate = pred_caption.split()\n",
    "    score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "    return score,real_caption,pred_caption"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "Test = True\n",
    "captionModel = CaptionModel()\n",
    "if (Test == True):\n",
    "    ds_val = ds_val.take(10)\n",
    "    for sample_img_batch, ground_truth, img_path in tqdm(ds_val):\n",
    "        # count += 1\n",
    "\n",
    "        tensor_shape = len(ground_truth.get_shape().as_list())\n",
    "        if(tensor_shape > 1):\n",
    "            ground_truth = tf.squeeze(ground_truth,[0])\n",
    "        ground_truth = list(set(ground_truth.numpy()))\n",
    "\n",
    "        img_path = img_path.numpy().decode()\n",
    "        h = head_model(sample_img_batch)\n",
    "        features, result = tail_model(h)\n",
    "        # features, result = model(sample_img_batch)\n",
    "        predictions, predictions_prob = get_predictions(cfg, result)\n",
    "        accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(cfg, imagesInfo, ground_truth,predictions, predictions_prob)\n",
    "\n",
    "        features = tf.reshape(features, [sample_img_batch.shape[0],8*8, 2048])\n",
    "        caption_tensor = captionModel.evaluate(features)\n",
    "\n",
    "        score,real_caption,pred_caption = process_caption_predictions(caption_tensor, img_path)\n",
    "\n",
    "        print(\"BLEU: %.2f\" % (score))\n",
    "        print ('Real:', real_caption)\n",
    "        print ('Pred:', pred_caption)    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 10%|█         | 1/10 [00:02<00:25,  2.87s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.60\n",
      "Real: a double decker bus is stopped along a curb\n",
      "Pred: a red double decker bus is driving down the street\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 20%|██        | 2/10 [00:03<00:13,  1.70s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.62\n",
      "Real: a man playing tennis on a red coat in all white\n",
      "Pred: a young girl playing tennis on a beach\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 30%|███       | 3/10 [00:04<00:10,  1.49s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.02\n",
      "Real: this video game has a wii device and instructions\n",
      "Pred: a   t a b l e   t o p p e d   w i t h   l o t s   o f   p e n s   a n d   p e n c i l   c a s e   w i t h   p e n s   a n d   p e n c i l   c a s e   w i t h   p e n s   a n d   p e n c i l   c a s e   w i t h   p e n s   a n d   p e n c i\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 40%|████      | 4/10 [00:05<00:07,  1.20s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.48\n",
      "Real: a man holds a sign advertising a deli\n",
      "Pred: a man walking down a street\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 50%|█████     | 5/10 [00:06<00:05,  1.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.62\n",
      "Real: a kitchen with hardwood floors and a sink and oven\n",
      "Pred: a kitchen with a stove and a counter\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 60%|██████    | 6/10 [00:07<00:03,  1.01it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.60\n",
      "Real: a stop sign on the side of a street\n",
      "Pred: a street with a street sign on a city street\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 70%|███████   | 7/10 [00:08<00:02,  1.04it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.45\n",
      "Real: these two elephants look like they are fighting\n",
      "Pred: an elephant standing next to a baby elephant in the dirt\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 80%|████████  | 8/10 [00:09<00:01,  1.11it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.34\n",
      "Real: there is a woman sitting outside writing and drinking coffee\n",
      "Pred: a woman is holding a laptop\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 90%|█████████ | 9/10 [00:10<00:00,  1.06it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.70\n",
      "Real: a view of a bunch of pizzas sitting on a table\n",
      "Pred: a table set with a lot of food and drinks\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLEU: 0.43\n",
      "Real: several mounted police officers and their horses line up on the street\n",
      "Pred: a group of jockeys on horses\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "for index, layer in enumerate(model_config['layers']):\n",
    "    if (np.shape(layer['inbound_nodes'])[0] > 0):\n",
    "        dim_1 = len(layer['inbound_nodes'][0])\n",
    "        if(dim_1 > 1):\n",
    "            print(index, layer['name'])\n",
    "            for i in range(dim_1):\n",
    "                print(\"    %s\" % (layer['inbound_nodes'][0][i][0]))\n",
    "                # print(tail_model_config['layers'][index]['inbound_nodes'][0][i])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40 mixed0\n",
      "    activation_5\n",
      "    activation_7\n",
      "    activation_10\n",
      "    activation_11\n",
      "63 mixed1\n",
      "    activation_12\n",
      "    activation_14\n",
      "    activation_17\n",
      "    activation_18\n",
      "86 mixed2\n",
      "    activation_19\n",
      "    activation_21\n",
      "    activation_24\n",
      "    activation_25\n",
      "100 mixed3\n",
      "    activation_26\n",
      "    activation_29\n",
      "    max_pooling2d_2\n",
      "132 mixed4\n",
      "    activation_30\n",
      "    activation_33\n",
      "    activation_38\n",
      "    activation_39\n",
      "164 mixed5\n",
      "    activation_40\n",
      "    activation_43\n",
      "    activation_48\n",
      "    activation_49\n",
      "196 mixed6\n",
      "    activation_50\n",
      "    activation_53\n",
      "    activation_58\n",
      "    activation_59\n",
      "228 mixed7\n",
      "    activation_60\n",
      "    activation_63\n",
      "    activation_68\n",
      "    activation_69\n",
      "248 mixed8\n",
      "    activation_71\n",
      "    activation_75\n",
      "    max_pooling2d_3\n",
      "276 mixed9_0\n",
      "    activation_78\n",
      "    activation_79\n",
      "277 concatenate\n",
      "    activation_82\n",
      "    activation_83\n",
      "279 mixed9\n",
      "    activation_76\n",
      "    mixed9_0\n",
      "    concatenate\n",
      "    activation_84\n",
      "307 mixed9_1\n",
      "    activation_87\n",
      "    activation_88\n",
      "308 concatenate_1\n",
      "    activation_91\n",
      "    activation_92\n",
      "310 mixed10\n",
      "    activation_85\n",
      "    mixed9_1\n",
      "    concatenate_1\n",
      "    activation_93\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coco_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py373",
   "language": "python",
   "name": "py373"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}