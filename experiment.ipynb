{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model\n",
    "from tensorflow.keras.activations import tanh\n",
    "from tensorflow.keras.activations import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.units=units\n",
    "        # build your Dense layer\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        # build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # build your final Dense layer with unit 1\n",
    "        # self.V = tf.keras.layers.Dense(1, activation='softmax')\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features shape: (batch_size, 8*8, embedding_dim)\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        score = tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        score = self.V(score)\n",
    "        attention_weights = softmax(score, axis=1)\n",
    "\n",
    "        # shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        # reduce the shape to (batch_size, embedding_dim)\n",
    "        # context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.reduce_mean(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 32\n",
    "attention = Attention_model(units)\n",
    "features = tf.zeros([32, 64, 2048], tf.float32)\n",
    "hidden = tf.zeros([32, 16], tf.float32)\n",
    "# output = attention(features,hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_list = [func for func in dir(Attention_model) if callable(getattr(Attention_model, func))]\n",
    "# method_list = [func for func in dir(Attention_model) if callable(getattr(Attention_model, func)) and not func.startswith(\"__\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-02112b679b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# attention()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py373/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2374\u001b[0m     \"\"\"\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2377\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "# attention()\n",
    "input_shape = (None, 32, 32, 3)\n",
    "\n",
    "model.build((32, 64, 2048),)\n",
    "attention.compile()\n",
    "attention.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d8a323e6010706682c07af791323eacfc072764aa514c33420848fded080be"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('py373': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}