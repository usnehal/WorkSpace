{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNK1f6BEEVR1"
   },
   "source": [
    "# EYE FOR BLIND\n",
    "This notebook will be used to prepare the capstone project 'Eye for Blind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rwy9sczpFOq9",
    "outputId": "121475e9-2892-4e5e-c261-c7a06338bc85"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Gd0q3ftWEVR7"
   },
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "import  time\n",
    "import  re\n",
    "import  glob\n",
    "from    time import sleep\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "from    skimage import io\n",
    "import  matplotlib.pyplot as plt\n",
    "import  random\n",
    "from    collections import Counter\n",
    "from    tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from    tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers,Model\n",
    "from    tqdm import tqdm\n",
    "from    nltk.translate.bleu_score import sentence_bleu\n",
    "import  socket\n",
    "import  pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf-Zt3C1EVSD"
   },
   "source": [
    "Let's read the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxY-ZCHXEVSE"
   },
   "source": [
    "## Data understanding\n",
    "1.Import the dataset and read image & captions into two seperate variables\n",
    "\n",
    "2.Visualise both the images & text present in the dataset\n",
    "\n",
    "3.Create word-to-index and index-to-word mappings.\n",
    "\n",
    "4.Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "5.Visualise the top 30 occuring words in the captions\n",
    "\n",
    "6.Create a list which contains all the captions & path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In WSL\n"
     ]
    }
   ],
   "source": [
    "in_nimble = False\n",
    "in_WSL = False\n",
    "in_tpu = False\n",
    "\n",
    "host = socket.gethostname()\n",
    "if('cuda' in host):\n",
    "    in_nimble = True\n",
    "    print(\"In NimbleBox\")\n",
    "if(host == 'LTsuphale-NC2JM'):\n",
    "    in_WSL = True\n",
    "    print(\"In WSL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jPM7st2EVSF",
    "outputId": "d2b7af7c-3f37-435f-e5cf-b8f96b372a86"
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the image into a seperate variable\n",
    "# images_path='Images'\n",
    "# text_file = './captions.txt'\n",
    "# images_path='/content/drive/MyDrive/TestImages'\n",
    "# text_file = '/content/drive/MyDrive/captions_small.txt'\n",
    "# images_path='/content/drive/MyDrive/Images/Images'\n",
    "# text_file = '/content/drive/MyDrive/captions.txt'\n",
    "# images_path='./Flickr8K/Images'\n",
    "# text_file = './Flickr8K/captions.txt'\n",
    "# images_path='TestImages'\n",
    "# text_file = './captions_small.txt'\n",
    "images_path = ''\n",
    "text_file = ''\n",
    "# 8, 128, 11828, 118287\n",
    "total_test_images = 128\n",
    "\n",
    "if(in_WSL == True):\n",
    "    images_path='/home/suphale/snehal_bucket/coco/raw-data/train2017/'\n",
    "if(in_nimble == True):\n",
    "    images_path='/mnt/disks/user/project/coco/train2017/'\n",
    "\n",
    "text_file = './lists/captions_' + str(total_test_images) + '.txt'\n",
    "list_file = './lists/images_' + str(total_test_images) + '.txt'\n",
    "# Find out total number of images in the images folder\n",
    "# all_imgs = glob.glob(images_path + '/*.jpg',recursive=True)\n",
    "# all_imgs = sorted(all_imgs)\n",
    "# total_num_images = len(all_imgs)\n",
    "# print(\"The total images present in the images folder: {}\".format(total_num_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total images present in the dataset: 128\n"
     ]
    }
   ],
   "source": [
    "all_imgs = [images_path + line.rstrip() for line in open(list_file)]\n",
    "all_imgs = sorted(all_imgs)\n",
    "total_num_images = len(all_imgs)\n",
    "print(\"The total images present in the dataset: {}\".format(total_num_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "FinyQD3kEVSJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total images present in the dataset: 128\n",
      "The total number of lines in the caption file: 641\n"
     ]
    }
   ],
   "source": [
    "#Visualise both the images & text present in the dataset\n",
    "print(\"The total images present in the dataset: {}\".format(total_num_images))\n",
    "num_lines = sum(1 for line in open(text_file))\n",
    "print(\"The total number of lines in the caption file: {}\".format(num_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bzryG2erEVSJ"
   },
   "outputs": [],
   "source": [
    "# define a function to clean text data\n",
    "def extract_jpg_caption(line):\n",
    "    char_filter = r\"[^\\w]\"\n",
    "\n",
    "    jpg_path = None\n",
    "    caption = None\n",
    "\n",
    "    jpg_position = line.find(\".jpg\")\n",
    "    if(jpg_position != -1):\n",
    "        jpg_path = images_path + '/' + line[:jpg_position+4]\n",
    "\n",
    "        caption = line[jpg_position+5:].strip()\n",
    "\n",
    "        # convert words to lower case\n",
    "        caption = caption.lower()\n",
    "\n",
    "        # split into words\n",
    "        words = caption.split()\n",
    "\n",
    "        # strip whitespace from all words\n",
    "        words = [word.strip() for word in words]\n",
    "\n",
    "        # join back words to get document\n",
    "        caption = \" \".join(words)\n",
    "\n",
    "        # remove unwanted characters\n",
    "        caption = re.sub(char_filter, \" \", caption)\n",
    "\n",
    "        # remove unwanted characters\n",
    "        caption = re.sub(r\"\\.\", \" \", caption)\n",
    "\n",
    "        # replace multiple whitespaces with single whitespace\n",
    "        caption = re.sub(r\"\\s+\", \" \", caption)\n",
    "\n",
    "        # strip whitespace from document\n",
    "        caption = caption.strip()\n",
    "\n",
    "        caption = '<start> ' + caption + ' <end>'\n",
    "\n",
    "    return jpg_path, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "nGF8eqJYEVSK"
   },
   "outputs": [],
   "source": [
    "#store all the image id here\n",
    "all_img_id= [] \n",
    "#store all the image path here\n",
    "all_img_vector=[]\n",
    "#store all the captions here\n",
    "annotations= [] \n",
    "# list of all captions in word list format\n",
    "annotations_word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "GwWfE7lMEVSK"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    #your code here\n",
    "    file  = open(filename, 'r') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    text = \"\"\n",
    "    count = 0\n",
    "    for line in Lines:\n",
    "        jpg_path, caption = extract_jpg_caption(line)\n",
    "        if(jpg_path != None):\n",
    "            all_img_id.append(count)\n",
    "            all_img_vector.append(jpg_path)\n",
    "            annotations.append(caption)\n",
    "            word_list = caption.split()\n",
    "            annotations_word_list.append(word_list)\n",
    "            text += \" \" + caption\n",
    "            count += 1\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8hQZDjOEVSL",
    "outputId": "67aee2a4-af6c-4412-8a94-dbc81cdeb146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <start> a large brown couch sitting next to a fireplace <end> <start> a living room has a sofa and pictures on the walls <end> <start> a beige sectional sofa next to a fireplace and a mirror <end> <start> a living room with a couch fireplace and wood floors <end> <start> an image of a living room s\n"
     ]
    }
   ],
   "source": [
    "#Import the dataset and read the text file into a seperate variable\n",
    "doc = load_doc(text_file)\n",
    "print(doc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words present in the captions = 1094\n"
     ]
    }
   ],
   "source": [
    "a = set()\n",
    "for x in annotations_word_list:\n",
    "    for y in x:\n",
    "        a.add(y)\n",
    "print(\"Total words present in the captions = %d\" % (len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBtVXKc_EVSL",
    "outputId": "6d6ce279-e7d6-4f74-dd92-5303962b86d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> a large brown couch sitting next to a fireplace <end>',\n",
       " '<start> a living room has a sofa and pictures on the walls <end>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVzMjVfCC6Xb"
   },
   "source": [
    "Randomly pick an image from the data and show its corresponding caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wugOWpmjEVSM",
    "outputId": "fb87beaa-4af0-44a7-c5e7-86a1353b37f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions present in the dataset: 640\n",
      "Total images present in the dataset: 640\n"
     ]
    }
   ],
   "source": [
    "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
    "print(\"Total images present in the dataset: \" + str(len(all_img_vector)))\n",
    "total_images = len(all_img_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXxFT_1fEVSN"
   },
   "source": [
    "Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJBf3xw1EVSO"
   },
   "source": [
    "## Pre-Processing the captions\n",
    "1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \n",
    "This gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n",
    "\n",
    "2.Replace all other words with the unknown token \"UNK\" .\n",
    "\n",
    "3.Create word-to-index and index-to-word mappings.\n",
    "\n",
    "4.Pad all sequences to be the same length as the longest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "dkhVhIP4EVSP"
   },
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "\n",
    "# your code here\n",
    "max_tokenized_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_tokenized_words+1,oov_token='<unknown>')\n",
    "tokenizer.fit_on_texts(annotations_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nX8PDZ3_EVSP",
    "outputId": "e538e663-d4de-42c3-fa3c-364a2c5755f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1096\n"
     ]
    }
   ],
   "source": [
    "# Create word-to-index and index-to-word mappings.\n",
    "VOCABULARY_SIZE = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(VOCABULARY_SIZE))\n",
    "\n",
    "# print(\"index of dog = %d\" % (tokenizer.word_index['dog']))\n",
    "# print(\"word corresponding to index 10 = %s\" % (tokenizer.index_word[10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "xCRGbE_fEVSP",
    "outputId": "27323304-5b68-4cd4-9606-f3cf54f744be"
   },
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions ^ store it to a vairable\n",
    "\n",
    "MAX_SEQ_LENGTH = 25\n",
    "\n",
    "cap_vector = annotations_word_list\n",
    "Y_encoded = tokenizer.texts_to_sequences(cap_vector)\n",
    "cap_vector_encoded_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-6Js9hxEVST"
   },
   "source": [
    "## Pre-processing the images\n",
    "\n",
    "1.Resize them into the shape of (299, 299)\n",
    "\n",
    "3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "QZ0EAZKLEVSU"
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "batch_size = 32\n",
    "def read_image(image_path,label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIdqOSlpEVSU"
   },
   "source": [
    "## Create the train & test data \n",
    "1.Combine both images & captions to create the train & test dataset using tf.data.Dataset API. Create the train-test spliit using 80-20 ratio & random state = 42\n",
    "\n",
    "2.Make sure you have done Shuffle and batch while building the dataset\n",
    "\n",
    "3.The shape of each image in the dataset after building should be (batch_size, 299, 299, 3)\n",
    "\n",
    "4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JxhdK2GGcQX"
   },
   "source": [
    "split the data into train, validation and final test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GkQryLPEVSQ",
    "outputId": "57d5c552-7bc7-4ad1-9043-bcc79056c467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images for training=506\n",
      "Total Images for testing (validation)=127\n",
      "Total Images for final random testing=7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train, validation and final test sets\n",
    "img_train, final_img_test, cap_train, final_cap_test = train_test_split(all_img_vector, cap_vector_encoded_padded, test_size=0.01, random_state=42)\n",
    "img_train, img_test, cap_train, cap_test = train_test_split(img_train, cap_train, test_size=0.2, random_state=42)\n",
    "\n",
    "total_training_images = len(img_train)\n",
    "total_test_images = len(img_test)\n",
    "total_final_testing_images = len(final_img_test)\n",
    "print(\"Total Images for training=%d\" % (total_training_images))\n",
    "print(\"Total Images for testing (validation)=%d\" % (total_test_images))\n",
    "print(\"Total Images for final random testing=%d\" % (total_final_testing_images))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "S6rCEjr3EVSR"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((img_train, cap_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((img_test, cap_test))\n",
    "final_test_dataset = tf.data.Dataset.from_tensor_slices((final_img_test, final_cap_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "C5sEcEB8EVSW"
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "train_dataset = train_dataset.map(read_image)\n",
    "train_dataset.shuffle(buffer_size=1024,reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(read_image)\n",
    "test_dataset.shuffle(buffer_size=1024,reshuffle_each_iteration=True)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj2ntOu6jHqU"
   },
   "source": [
    "A utility function to display image retrieved from a dataset and its corresponding caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "hKTzT0RFEVSY"
   },
   "outputs": [],
   "source": [
    "# A utility function to display image retrieved from a dataset and its corresponding caption\n",
    "def show_image_caption_from_dataset(image,label):\n",
    "    plt.imshow(image)\n",
    "    for x in label.numpy():\n",
    "        if(x != 0):\n",
    "            print(tokenizer.index_word[x], end =\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhYg1RsGjcEc"
   },
   "source": [
    "Check that the data retrieved from dataset is in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "LoAsEok1EVSZ",
    "outputId": "f47af2f3-7fb9-47b8-d8b9-cd7ea323490b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 299, 299, 3)\n",
      "(32, 25)\n"
     ]
    }
   ],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "# (batch_size, 299, 299, 3)\n",
    "print(sample_img_batch.shape) \n",
    "# (batch_size, max_len)\n",
    "print(sample_cap_batch.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "biEYWS1cEVSZ",
    "outputId": "850505f2-d96f-486b-94ff-8202daa04fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 299, 299, 3)\n",
      "(32, 25)\n"
     ]
    }
   ],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(test_dataset))\n",
    "# (batch_size, 299, 299, 3)\n",
    "print(sample_img_batch.shape) \n",
    "# (batch_size, max_len)\n",
    "print(sample_cap_batch.shape) \n",
    "# show_image_caption_from_dataset(sample_img_batch[0],sample_cap_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6gbmcC3EVSa"
   },
   "source": [
    "## Load the pretrained Imagenet weights of Inception net V3\n",
    "\n",
    "1.To save the memory(RAM) from getting exhausted, extract the features of thei mage using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n",
    "\n",
    "2.The shape of the output of this layer is 8x8x2048. \n",
    "\n",
    "3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "oNZnAGSsEVSb"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "# write code here to get the input of the image_model\n",
    "new_input = image_model.input \n",
    "# write code here to get the output of the image_model\n",
    "hidden_layer = image_model.output \n",
    "\n",
    "#build the final model using both input & output layer\n",
    "image_features_extract_model = keras.Model(inputs=new_input, outputs=hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in image_model.layers[:-1]:\n",
    "#     print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64 \n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(268203, activation='sigmoid'),\n",
    "      layers.Reshape((299, 299, 3))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "autoencoder = Autoencoder(latent_dim) \n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_10 (Sequential)   (299, 64)                 57472     \n",
      "_________________________________________________________________\n",
      "sequential_11 (Sequential)   (299, 299, 299, 3)        17433195  \n",
      "=================================================================\n",
      "Total params: 17,490,667\n",
      "Trainable params: 17,490,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.build((299,299,3))\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import  losses\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(3,), dtype=int32)\n",
      "Epoch 1/10\n",
      "506/506 [==============================] - 1634s 3s/step - loss: -1.4079\n",
      "Epoch 2/10\n",
      " 22/506 [>.............................] - ETA: 27:18 - loss: -1.6425"
     ]
    }
   ],
   "source": [
    "#write your code here\n",
    "batch_size = 32\n",
    "def my_read_image(image_path,label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "    print(tf.shape(image))\n",
    "    return image, image\n",
    "\n",
    "ae_train_dataset = tf.data.Dataset.from_tensor_slices((img_train, img_train))\n",
    "# ae_test_dataset = tf.data.Dataset.from_tensor_slices((img_test, img_test))\n",
    "# ae_train_dataset = ae_train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "ae_train_dataset = ae_train_dataset.map(my_read_image)\n",
    "# ae_train_dataset.shuffle(buffer_size=1024,reshuffle_each_iteration=True)\n",
    "# ae_train_dataset = ae_train_dataset.batch(batch_size)\n",
    "# ae_train_dataset = ae_train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# print(type(ae_train_dataset))\n",
    "# print(str(ae_train_dataset.shape()))\n",
    "\n",
    "autoencoder.fit(ae_train_dataset,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K34JkKjtEVSb"
   },
   "outputs": [],
   "source": [
    "# write your code to extract features from each image in the dataset\n",
    "def extract_image_features(sample_img_batch):\n",
    "    features = image_features_extract_model(sample_img_batch)\n",
    "    features = tf.reshape(features, [sample_img_batch.shape[0],8*8, 2048])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmmaxLLREVSe",
    "outputId": "78e141c8-4b94-4665-ec1e-50c5357eb2c6"
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "sample_img_batch = extract_image_features(sample_img_batch)\n",
    "# (batch_size, 8*8, 2048)\n",
    "print(\"sample_img_batch.shape after extract_image_features\",sample_img_batch.shape)  \n",
    "# (batch_size,40)\n",
    "print(\"sample_cap_batch.shape after extract_image_features\",sample_cap_batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vztEm1vrEVSe",
    "outputId": "52263984-47c4-47bf-9683-614f29ef397a"
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(test_dataset))\n",
    "sample_img_batch = extract_image_features(sample_img_batch)\n",
    "# (batch_size, 8*8, 2048)\n",
    "print(\"sample_img_batch.shape after extract_image_features\",sample_img_batch.shape)  \n",
    "# (batch_size,40)\n",
    "print(\"sample_cap_batch.shape after extract_image_features\",sample_cap_batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyu7txNtEVSf"
   },
   "source": [
    "## Model Building\n",
    "1.Set the parameters\n",
    "\n",
    "2.Build the Encoder, Attention model & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrdrgKbTEVSf"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256 \n",
    "units = 512\n",
    "vocab_size = max_tokenized_words + 1\n",
    "train_num_steps = total_training_images //batch_size #len(total train images) // BATCH_SIZE\n",
    "test_num_steps = total_test_images //batch_size #len(total test images) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rq0rRVQEVSg"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pgG7OkBEVSg"
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # build your Dense layer with relu activation\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation='relu')\n",
    "        \n",
    "    def call(self, features):\n",
    "        # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "        features = self.dense(features)\n",
    "        return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnNo7xm7EVSg"
   },
   "outputs": [],
   "source": [
    "encoder=Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5d6VuSdEVSg"
   },
   "source": [
    "### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D02WLWNNEVSg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import tanh\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.units=units\n",
    "        # build your Dense layer\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        # build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # build your final Dense layer with unit 1\n",
    "        # self.V = tf.keras.layers.Dense(1, activation='softmax')\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features shape: (batch_size, 8*8, embedding_dim)\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        score = tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        score = self.V(score)\n",
    "        attention_weights = softmax(score, axis=1)\n",
    "\n",
    "        # shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        # reduce the shape to (batch_size, embedding_dim)\n",
    "        # context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.reduce_mean(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITGBKawcEVSh"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yP8k2UdbEVSh"
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        # iniitalise your Attention model with units\n",
    "        self.attention = Attention_model(self.units)\n",
    "        # build your Embedding layer\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        # build your Dense layer\n",
    "        self.d1 = tf.keras.layers.Dense(self.units)\n",
    "        # build your Dense layer\n",
    "        self.d2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        #create your context vector & attention weights from attention model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed = self.embed(x)\n",
    "        # Concatenate your input with the context vector from attention layer. \n",
    "        # Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)\n",
    "        # Extract the output & hidden state from GRU layer. \n",
    "        # Output shape : (batch_size, max_length, hidden_size)\n",
    "        output, state = self.gru(embed)\n",
    "        output = self.d1(output)\n",
    "        # shape : (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) \n",
    "        # shape : (batch_size * max_length, vocab_size)\n",
    "        output = self.d2(output)\n",
    "\n",
    "        return output, state, attention_weights\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHU0wjNaEVSh"
   },
   "outputs": [],
   "source": [
    "decoder=Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCfPNkjSEVSh",
    "outputId": "81a75bd3-ad2a-4665-e9cb-19ef4d186514"
   },
   "outputs": [],
   "source": [
    "features=encoder(sample_img_batch)\n",
    "\n",
    "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
    "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
    "\n",
    "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
    "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
    "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
    "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT_b48HEEVSi"
   },
   "source": [
    "## Model training & optimization\n",
    "1.Set the optimizer & loss object\n",
    "\n",
    "2.Create your checkpoint path\n",
    "\n",
    "3.Create your training & testing step functions\n",
    "\n",
    "4.Create your loss function for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQSaRpQQEVSi"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8zsYc0_EVSi"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zxPDixbEVSj"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./ckpt\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GppbW39tEVSk"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDVWr-jjEVSl"
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    img_tensor = extract_image_features(img_tensor)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the fixed length vector of Encoder-Decoder model\n",
    "        features = encoder(img_tensor)\n",
    "        # Loop through for max_length times\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Get predictions from the decoder\n",
    "            # print(\"dec_input=%s features=%s hidden=%s\" % (str(type(dec_input)), str(type(features)), str(type(hidden))))\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Get the next target vector as dec_input\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nDn-xmOEVSn"
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def test_step(img_tensor, target):\n",
    "    #write your code here to do the testing steps\n",
    "    loss = 0\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    img_tensor = extract_image_features(img_tensor)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "        loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "        dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2s6DLVNEVSn"
   },
   "outputs": [],
   "source": [
    "def test_loss_cal(test_dataset):\n",
    "    total_loss = 0\n",
    "\n",
    "    #write your code to get the average loss result on your test data\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n",
    "        batch_loss, t_loss = test_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_test_loss=total_loss / test_num_steps\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iw_rObMKEVSn",
    "outputId": "e3dc3b43-65e9-49d0-ea03-055dac1ac4a3"
   },
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "test_loss_plot = []\n",
    "EPOCHS = 10\n",
    "\n",
    "best_test_loss=100\n",
    "for epoch in tqdm(range(0, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_train_loss=total_loss / train_num_steps\n",
    "        \n",
    "    loss_plot.append(avg_train_loss)    \n",
    "\n",
    "    test_loss = test_loss_cal(test_dataset)\n",
    "    test_loss_plot.append(test_loss)\n",
    "    \n",
    "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
    "        best_test_loss = test_loss\n",
    "        ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "50gx5tp5EVSo",
    "outputId": "6c6a9b99-be0e-4e54-e152-46fc8b8875cb"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.plot(test_loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gpw0k-mhEVSo"
   },
   "source": [
    "## Model Evaluation\n",
    "1.Define your evaluation function using greedy search\n",
    "\n",
    "2.Define your evaluation function using beam search ( optional)\n",
    "\n",
    "3.Test it on a sample data using BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmbqy_0XEVSo"
   },
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTpgLUbTEVSp"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_features_shape = 64\n",
    "    attention_plot = np.zeros((MAX_SEQ_LENGTH, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    # process the input image to desired format before extracting features\n",
    "    temp_input = tf.expand_dims(read_image(image,[])[0], 0) \n",
    "    # Extract features using our feature extraction model\n",
    "    img_tensor_val = extract_image_features(temp_input)\n",
    "    # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    # extract the features by passing the input to encoder\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(MAX_SEQ_LENGTH):\n",
    "        # get the output from decoder\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        #extract the predicted id(embedded value) which carries the max value\n",
    "        predicted_id = tf.argmax(tf.transpose(predictions))\n",
    "        predicted_id = predicted_id.numpy()[0]\n",
    "        # map the id to the word from tokenizer and append the value to the result list\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot,predictions\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot,predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSMhRhbIEVSp"
   },
   "source": [
    "### Beam Search(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DTtqauGEVSp"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate(image, beam_index = 0): #your value for beam index):\n",
    "\n",
    "    final_caption = \"ToDo\"\n",
    "    #write your code to evaluate the result using beam search\n",
    "                  \n",
    "    return final_caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNm6XDyhEVSq"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def plot_attmap(caption, weights, image):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    temp_img = np.array(Image.open(image))\n",
    "    \n",
    "    len_cap = len(caption)\n",
    "    for cap in range(len_cap):\n",
    "        weights_img = np.reshape(weights[cap], (8,8))\n",
    "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
    "        \n",
    "        ax = fig.add_subplot((len_cap//2)+1, (len_cap//2)+1, cap+1)\n",
    "        ax.set_title(caption[cap], fontsize=15)\n",
    "        \n",
    "        img=ax.imshow(temp_img)\n",
    "        \n",
    "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
    "        ax.axis('off')\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mweLMXVAEVSq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLXHjJmGEVSq"
   },
   "outputs": [],
   "source": [
    "def filt_text(text):\n",
    "    filt=['<start>','<unk>','<end>'] \n",
    "    temp= text.split()\n",
    "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
    "    text=' '.join(temp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "nUEIaCdXEVSq",
    "outputId": "2363d7ee-7c53-420b-e7ce-688d7f8377aa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "rid = np.random.randint(0, total_final_testing_images)\n",
    "test_image = final_img_test[rid]\n",
    "# test_image = './Images/413231421_43833a11f5.jpg'\n",
    "# test_image = '/content/drive/MyDrive/TestImages/3637013_c675de7705.jpg'\n",
    "\n",
    "real_caption = final_cap_test[rid]\n",
    "# real_caption = '<start> A couple stands close at the water edge <end>'\n",
    "# real_caption = '<start> black dog is digging in the snow <end>'\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in final_cap_test[rid] if i not in [0]])\n",
    "\n",
    "t0= time.perf_counter()\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "t1 = time.perf_counter() - t0\n",
    "print(\"Time elapsed: \", t1)\n",
    "\n",
    "real_caption=filt_text(real_caption)      \n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "print(f\"BLEU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', pred_caption)\n",
    "plot_attmap(result, attention_plot, test_image)\n",
    "\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5v7_8mEEVSq"
   },
   "outputs": [],
   "source": [
    "captions=beam_evaluate(test_image)\n",
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjLXMrn8EVSr"
   },
   "outputs": [],
   "source": [
    "# image_features_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4ZUTC3xEVSr"
   },
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('temp_model/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#decoder.save('MyModel',save_format='tf')\n",
    "model_json = image_features_extract_model.to_json()\n",
    "with open(\"temp_model/image_features_extract_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "image_features_extract_model.save_weights(\"temp_model/image_features_extract_model.h5\")\n",
    "print(\"Saved image_features_extract_model to disk\")\n",
    "\n",
    "decoder.save_weights(\"temp_model/decoder.h5\")\n",
    "encoder.save_weights(\"temp_model/encoder.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('temp_model/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "#Reading the model from JSON file\n",
    "with open('temp_model/image_features_extract_model.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "#load the model architecture \n",
    "model_j = tf.keras.models.model_from_json(json_savedModel)\n",
    "model_j.load_weights('temp_model/image_features_extract_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.load_weights(\"temp_model/decoder.h5\")\n",
    "encoder.load_weights(\"temp_model/encoder.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "test_image = './413231421_43833a11f5.jpg'\n",
    "# test_image = '/content/drive/MyDrive/TestImages/3637013_c675de7705.jpg'\n",
    "\n",
    "# real_caption = final_cap_test[rid]\n",
    "real_caption = '<start> A couple stands close at the water edge <end>'\n",
    "# real_caption = '<start> black dog is digging in the snow <end>'\n",
    "# real_caption = ' '.join([tokenizer.index_word[i] for i in final_cap_test[rid] if i not in [0]])\n",
    "\n",
    "t0= time.perf_counter()\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "t1 = time.perf_counter() - t0\n",
    "print(\"Time elapsed: \", t1)\n",
    "\n",
    "real_caption=filt_text(real_caption)      \n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "print(f\"BLEU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', pred_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cap_01.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "53d8a323e6010706682c07af791323eacfc072764aa514c33420848fded080be"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('py373': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}