{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Import all the required libraries\n",
    "import  time\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "# from    skimage import io\n",
    "# import  random\n",
    "# from    collections import Counter\n",
    "# from    tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import  tensorflow as tf\n",
    "# from    tensorflow import keras\n",
    "# from    tensorflow.keras import layers,Model\n",
    "from    tqdm import tqdm\n",
    "# from    nltk.translate.bleu_score import sentence_bleu\n",
    "# import  socket\n",
    "# import  pickle5 as pickle\n",
    "# from    tensorflow.keras.activations import tanh\n",
    "# from    tensorflow.keras.activations import softmax\n",
    "# import  matplotlib.pyplot as plt\n",
    "import  time\n",
    "import  argparse\n",
    "from    sklearn.metrics import accuracy_score\n",
    "import  tensorflow_datasets as tfds\n",
    "import  functools\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from Helper import Config, ImagesInfo, Logger, Client, TimeKeeper\n",
    "from Helper import read_image\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-s', '--server', action='store', type=str, required=False)\n",
    "parser.add_argument('-t', '--test_number', action='store', type=int, required=False)\n",
    "parser.add_argument('-v', '--verbose', action='store', type=int, required=False)\n",
    "parser.add_argument('-i', '--image_size', action='store', type=int, required=False)\n",
    "parser.add_argument('-m', '--max_tests', action='store', type=int, required=False)\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(args.server)\n",
    "\n",
    "server_ip = args.server\n",
    "test_number = args.test_number\n",
    "verbose = args.verbose\n",
    "imagesize = args.imagesize\n",
    "max_tests = args.max_tests\n",
    "\n",
    "if(verbose == None):\n",
    "    verbose = 1\n",
    "\n",
    "if(test_number == None):\n",
    "    test_number = 3\n",
    "\n",
    "if(max_tests == None):\n",
    "    max_tests = 100\n",
    "\n",
    "test_scenarios = {  1:\"Complete jpg file buffer transfer\", \n",
    "                    2:\"Decoded image buffer transfer\",\n",
    "                    3:\"Decoded image buffer transfer with zlib compression\"}\n",
    "\n",
    "if(imagesize == None):\n",
    "    imagesize = 299\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "Test scenario = 3 Decoded image buffer transfer with zlib compression\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Logger.set_log_level(verbose)\n",
    "tk = TimeKeeper()\n",
    "cfg = Config(server_ip)\n",
    "client = Client(cfg)\n",
    "imagesInfo = ImagesInfo(cfg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "total_test_images = 100\n",
    "batch_size = 32\n",
    "PREDICTIONS_THRESHOLD = 0.4\n",
    "h_image_height = imagesize\n",
    "h_image_width = imagesize\n",
    "\n",
    "Logger.milestone_print(\"Test scenario = %d %s\" % (test_number, test_scenarios[test_number]))\n",
    "Logger.milestone_print(\"Image shape = (%d %d)\" % (h_image_height, h_image_width))\n",
    "Logger.milestone_print(\"Max tests = %d\" % (max_tests))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# new_head_model = tf.keras.models.load_model(cfg.temp_path + '/new_head_model')\n",
    "# new_tail_model = tf.keras.models.load_model(cfg.temp_path + '/new_tail_model')\n",
    "\n",
    "new_head_model = tf.keras.models.load_model(cfg.saved_model_path + '/head_model')\n",
    "new_tail_model = tf.keras.models.load_model(cfg.saved_model_path + '/tail_model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def process_predictions(ground_truth, prediction_tensor):\n",
    "    n = tf.squeeze(prediction_tensor).numpy()\n",
    "    df = pd.DataFrame(columns=['id_index','probability'])\n",
    "    predictions_str = ''\n",
    "    top_predictions = []\n",
    "    index = 0\n",
    "    for x in n:\n",
    "        if x > PREDICTIONS_THRESHOLD:\n",
    "            top_predictions.append(index)\n",
    "            predictions_str += \"%s(%.2f) \" % (imagesInfo.classes[index],x)\n",
    "            df = df.append({'id_index':int(index), 'probability':x},ignore_index = True)\n",
    "        index += 1\n",
    "\n",
    "    df = df.sort_values('probability', ascending=False)\n",
    "    sorted_predictions = df['id_index'].tolist()\n",
    "    sorted_predictions = [int(x) for x in sorted_predictions]\n",
    "\n",
    "    ground_truth_length = len(ground_truth)\n",
    "    predictions_length = len(sorted_predictions)\n",
    "\n",
    "    aligned_predictions = [-1] * ground_truth_length\n",
    "    TP = 0\n",
    "    for i in range(ground_truth_length):\n",
    "        if(ground_truth[i] in sorted_predictions):\n",
    "            aligned_predictions[i] = ground_truth[i]\n",
    "            TP += 1\n",
    "    accuracy = accuracy_score(ground_truth, aligned_predictions)\n",
    "\n",
    "    top_1_accuracy = 0.0\n",
    "    top_5_accuracy = 0.0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    if(predictions_length > 0):\n",
    "        if(sorted_predictions[0] in ground_truth):\n",
    "            top_1_accuracy = 1.0\n",
    "        for i in range(5):\n",
    "            if((i < predictions_length) and (sorted_predictions[i] in ground_truth)):\n",
    "                top_5_accuracy = 1.0\n",
    "\n",
    "        precision = TP / predictions_length\n",
    "    if(predictions_length > 0):\n",
    "        recall = TP / ground_truth_length\n",
    "    return accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def evaluate_classification(image):\n",
    "    temp_input = tf.expand_dims(read_image(image), 0) \n",
    "    h = new_head_model(temp_input)\n",
    "    s = new_tail_model(h)\n",
    "    return s\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "data_dir='/home/suphale/coco'\n",
    "N_LABELS = 80\n",
    "split_val = \"validation[:20%]\"\n",
    "# h_image_height = 299\n",
    "# h_image_width = 299\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class BoxField:\n",
    "    BOXES = 'bbox'\n",
    "    KEYPOINTS = 'keypoints'\n",
    "    LABELS = 'label'\n",
    "    MASKS = 'masks'\n",
    "    NUM_BOXES = 'num_boxes'\n",
    "    SCORES = 'scores'\n",
    "    WEIGHTS = 'weights'\n",
    "\n",
    "class DatasetField:\n",
    "    IMAGES = 'images'\n",
    "    IMAGES_INFO = 'images_information'\n",
    "    IMAGES_PMASK = 'images_padding_mask'\n",
    "\n",
    "def my_preprocess(inputs):\n",
    "    image = inputs['image']\n",
    "    image = tf.image.resize(image, (h_image_height, h_image_width))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "\n",
    "    targets = inputs['objects']\n",
    "\n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    inputs = {DatasetField.IMAGES: image, DatasetField.IMAGES_INFO: image_information}\n",
    "\n",
    "    # ground_truths = {\n",
    "    #     BoxField.BOXES: targets[BoxField.BOXES] * tf.tile(image_information[tf.newaxis], [1, 2]),\n",
    "    #     BoxField.LABELS: tf.cast(targets[BoxField.LABELS], tf.int32),\n",
    "    #     BoxField.NUM_BOXES: tf.shape(targets[BoxField.LABELS]),\n",
    "    #     BoxField.WEIGHTS: tf.fill(tf.shape(targets[BoxField.LABELS]), 1.0)\n",
    "    # }\n",
    "    ground_truths = tf.cast(targets[BoxField.LABELS], tf.int32)\n",
    "    # ground_truths = tf.one_hot(ground_truths, depth=N_LABELS, dtype=tf.int32)\n",
    "    # ground_truths = tf.reduce_sum(ground_truths, 0)\n",
    "    # ground_truths = tf.greater( ground_truths, tf.constant( 0 ) )    \n",
    "    # ground_truths = tf.where (ground_truths, 1, 0) \n",
    "    return image, ground_truths\n",
    "\n",
    "def expand_dims_for_single_batch(image, ground_truths):\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    ground_truths = tf.expand_dims(ground_truths, axis=0)\n",
    "    return image, ground_truths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ds_val = tfds.load(name=\"coco/2017\", split=split_val, data_dir=data_dir, shuffle_files=False, download=False)\n",
    "ds_val = ds_val.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# ds_val = ds_val.range(100)\n",
    "\n",
    "# iterator = ds_val.make_one_shot_iterator()\n",
    "\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f8839e2c830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f8839e2c830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function my_preprocess at 0x7f8839e2c830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df = pd.DataFrame(columns=['img_path','ground_truth', 'top_predict', 'Prediction', 'accuracy', 'top_1_accuracy', 'top_5_accuracy', 'precision', 'recall', 'time'])\n",
    "\n",
    "total_time = 0.0\n",
    "# for test_index in range(10):\n",
    "count = 0\n",
    "max_test_images = max_tests\n",
    "# for sample_img_batch, ground_truth in ds_val:\n",
    "for i in tqdm(range(max_test_images)):\n",
    "# for sample_img_batch, ground_truth in ds_val:\n",
    "    count += 1\n",
    "    sample_img_batch, ground_truth = next(iter(ds_val))\n",
    "    tensor_shape = len(ground_truth.get_shape().as_list())\n",
    "    if(tensor_shape > 1):\n",
    "        ground_truth = tf.squeeze(ground_truth,[0])\n",
    "    # print(ground_truth)\n",
    "    t0= time.perf_counter()\n",
    "    h = new_head_model(sample_img_batch)\n",
    "    s = new_tail_model(h)\n",
    "    t1 = time.perf_counter() - t0\n",
    "    total_time = total_time + t1\n",
    "\n",
    "    accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(ground_truth,s)\n",
    "    df = df.append(\n",
    "        {'image':i, \n",
    "        'ground_truth':(str(imagesInfo.get_segmentation_texts(ground_truth))),\n",
    "        'top_predict':str(top_predictions),\n",
    "        'Prediction':predictions_str,\n",
    "        'accuracy':accuracy,\n",
    "        'top_1_accuracy':top_1_accuracy,\n",
    "        'top_5_accuracy':top_5_accuracy,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'time':t1,\n",
    "        },\n",
    "        ignore_index = True)\n",
    "    truth_str = ' '.join([str(elem) for elem in imagesInfo.get_segmentation_texts(ground_truth)])\n",
    "    Logger.debug_print(\"ground_truth  : %s\" % (truth_str))\n",
    "    Logger.debug_print(\"Prediction    : %s\" % (predictions_str))\n",
    "\n",
    "df.to_csv(cfg.temp_path + '/results_'+cfg.timestr+'.csv')\n",
    "av_column = df.mean(axis=0)\n",
    "\n",
    "print(count)\n",
    "\n",
    "Logger.milestone_print(\"accuracy        : %.2f\" % (av_column.accuracy))\n",
    "Logger.milestone_print(\"top_1_accuracy  : %.2f\" % (av_column.top_1_accuracy))\n",
    "Logger.milestone_print(\"top_5_accuracy  : %.2f\" % (av_column.top_5_accuracy))\n",
    "Logger.milestone_print(\"precision       : %.2f\" % (av_column.precision))\n",
    "Logger.milestone_print(\"recall          : %.2f\" % (av_column.recall))\n",
    "Logger.milestone_print(\"time            : %.2f\" % (av_column.time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/suphale/anaconda3/envs/py373/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n",
      "\u001b[92maccuracy        : 0.49\u001b[0m\n",
      "\u001b[92mtop_1_accuracy  : 0.71\u001b[0m\n",
      "\u001b[92mtop_5_accuracy  : 0.76\u001b[0m\n",
      "\u001b[92mprecision       : 1.98\u001b[0m\n",
      "\u001b[92mrecall          : 0.49\u001b[0m\n",
      "\u001b[92mtime            : 0.46\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}