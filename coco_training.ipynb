{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "#!pip install --upgrade git+https://github.com/EmGarr/kerod.git"
   ],
   "outputs": [],
   "metadata": {
    "id": "1jVKNjhdLFUQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ],
   "outputs": [],
   "metadata": {
    "id": "CET-72i5EmKn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([1,2,1,2,3,4,2])\n",
    "x = tf.expand_dims(x, axis=0)\n",
    "x = tf.tile(x, [x.shape[1], 1])\n",
    "x_ = tf.transpose(x)\n",
    "Y = tf.where(tf.equal(x,x_), tf.ones_like(x), tf.zeros_like(x))\n",
    "Y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=int32, numpy=\n",
       "array([[1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1]], dtype=int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download COCO/2017\n",
    "\n",
    "Download and preprocess COCO/2017 to the following format (required by od networks):\n",
    "\n",
    "```python\n",
    "dataset = {\n",
    "        'images' : A tensor of float32 and shape [1, height, widht, 3],\n",
    "        'images_info': A tensor of float32 and shape [1, 2] ,\n",
    "        'bbox': A tensor of float32 and shape [1, num_boxes, 4],\n",
    "        'labels': A tensor of int32 and shape [1, num_boxes],\n",
    "        'num_boxes': A tensor of int32 and shape [1, 1],\n",
    "        'weights': A tensor of float32 and shape [1, num_boxes]\n",
    "    }\n",
    "```\n",
    "\n",
    "If you need to download the dataset in a specific directory you can use the argument `data_dir` of `tfds.load`."
   ],
   "metadata": {
    "id": "Ews_7qF9OBi8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from   tensorflow.keras.utils import to_categorical\n",
    "import  matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "# from kerod.dataset.preprocessing import preprocess, expand_dims_for_single_batch\n",
    "# from kerod.core.standard_fields import BoxField\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def my_read_image(image_path,label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "    return image, image\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", data_dir='/home/suphale/coco', shuffle_files=True, with_info=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "class BoxField:\n",
    "    BOXES = 'bbox'\n",
    "    KEYPOINTS = 'keypoints'\n",
    "    LABELS = 'label'\n",
    "    MASKS = 'masks'\n",
    "    NUM_BOXES = 'num_boxes'\n",
    "    SCORES = 'scores'\n",
    "    WEIGHTS = 'weights'\n",
    "\n",
    "\n",
    "class DatasetField:\n",
    "    IMAGES = 'images'\n",
    "    IMAGES_INFO = 'images_information'\n",
    "    IMAGES_PMASK = 'images_padding_mask'\n",
    "\n",
    "def my_preprocess(inputs):\n",
    "    # tf.print(\"my_preprocess: type(input) %s\" % (str(type(inputs))))\n",
    "    # print(\"my_preprocess: type(x) %s\" % (str(type(x))))\n",
    "    # tf.print(inputs['image/filename'])\n",
    "    image = inputs['image']\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "\n",
    "    targets = inputs['objects']\n",
    "\n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    inputs = {DatasetField.IMAGES: image, DatasetField.IMAGES_INFO: image_information}\n",
    "\n",
    "    # ground_truths = {\n",
    "    #     BoxField.BOXES: targets[BoxField.BOXES] * tf.tile(image_information[tf.newaxis], [1, 2]),\n",
    "    #     BoxField.LABELS: tf.cast(targets[BoxField.LABELS], tf.int32),\n",
    "    #     BoxField.NUM_BOXES: tf.shape(targets[BoxField.LABELS]),\n",
    "    #     BoxField.WEIGHTS: tf.fill(tf.shape(targets[BoxField.LABELS]), 1.0)\n",
    "    # }\n",
    "    ground_truths = tf.cast(targets[BoxField.LABELS], tf.int32)\n",
    "    # tf.print(tf.shape(ground_truths))\n",
    "    ground_truths = tf.one_hot(ground_truths, depth=80, dtype=tf.int32)\n",
    "    # tf.print(tf.shape(ground_truths))\n",
    "    ground_truths = tf.reduce_sum(ground_truths, 0)\n",
    "    # tf.print(tf.shape(ground_truths))\n",
    "    return image, ground_truths\n",
    "    # return inputs\n",
    "\n",
    "def expand_dims_for_single_batch(image, ground_truths):\n",
    "\n",
    "    # tf.print(tf.shape(image))\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    ground_truths = tf.expand_dims(ground_truths, axis=0)\n",
    "    return image, ground_truths\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# ds_train = ds_train.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# sample_img_batch, sample_cap_batch = next(iter(ds_train))\n",
    "# plt.imshow(sample_img_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# tf.argmax(sample_cap_batch[0])\n",
    "# sample_cap_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "data_dir='/home/suphale/coco'\n",
    "\n",
    "ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", data_dir=data_dir, shuffle_files=True, download=False, with_info=True)\n",
    "ds_train = ds_train.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Filter example with no boxes after preprocessing\n",
    "# ds_train =  ds_train.filter(lambda x, y: tf.shape(y[BoxField.BOXES])[0] > 1)\n",
    "ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = tfds.load(name=\"coco/2017\", split=\"validation\", data_dir=data_dir, shuffle_files=False, download=False)\n",
    "ds_test = ds_test.map(\n",
    "    functools.partial(my_preprocess),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Filter example with no boxes after preprocessing\n",
    "# ds_test =  ds_test.filter(lambda x, y: tf.shape(y[BoxField.BOXES])[0] > 1)\n",
    "ds_test = ds_test.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f01ff217830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f01ff217830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function my_preprocess at 0x7f01ff217830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "metadata": {
    "id": "1cC2k8osNGFw",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "ds_info"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='coco',\n",
       "    full_name='coco/2017/1.1.0',\n",
       "    description=\"\"\"\n",
       "    COCO is a large-scale object detection, segmentation, and\n",
       "    captioning dataset.\n",
       "    \n",
       "    Note:\n",
       "     * Some images from the train and validation sets don't have annotations.\n",
       "     * Coco 2014 and 2017 uses the same images, but different train/val/test splits\n",
       "     * The test split don't have any annotations (only images).\n",
       "     * Coco defines 91 classes but the data only uses 80 classes.\n",
       "     * Panotptic annotations defines defines 200 classes but only uses 133.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    \n",
       "    This version contains images, bounding boxes and labels for the 2017 version.\n",
       "    \n",
       "    \"\"\",\n",
       "    homepage='http://cocodataset.org/#home',\n",
       "    data_path='/home/suphale/coco/coco/2017/1.1.0',\n",
       "    download_size=25.20 GiB,\n",
       "    dataset_size=24.98 GiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "        'image/filename': Text(shape=(), dtype=tf.string),\n",
       "        'image/id': tf.int64,\n",
       "        'objects': Sequence({\n",
       "            'area': tf.int64,\n",
       "            'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
       "            'id': tf.int64,\n",
       "            'is_crowd': tf.bool,\n",
       "            'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80),\n",
       "        }),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=40670, num_shards=64>,\n",
       "        'train': <SplitInfo num_examples=118287, num_shards=256>,\n",
       "        'validation': <SplitInfo num_examples=5000, num_shards=8>,\n",
       "    },\n",
       "    citation=\"\"\"@article{DBLP:journals/corr/LinMBHPRDZ14,\n",
       "      author    = {Tsung{-}Yi Lin and\n",
       "                   Michael Maire and\n",
       "                   Serge J. Belongie and\n",
       "                   Lubomir D. Bourdev and\n",
       "                   Ross B. Girshick and\n",
       "                   James Hays and\n",
       "                   Pietro Perona and\n",
       "                   Deva Ramanan and\n",
       "                   Piotr Doll{'{a}}r and\n",
       "                   C. Lawrence Zitnick},\n",
       "      title     = {Microsoft {COCO:} Common Objects in Context},\n",
       "      journal   = {CoRR},\n",
       "      volume    = {abs/1405.0312},\n",
       "      year      = {2014},\n",
       "      url       = {http://arxiv.org/abs/1405.0312},\n",
       "      archivePrefix = {arXiv},\n",
       "      eprint    = {1405.0312},\n",
       "      timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},\n",
       "      biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},\n",
       "      bibsource = {dblp computer science bibliography, https://dblp.org}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {
    "id": "Fip6g2i-B3pi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and train the network\n"
   ],
   "metadata": {
    "id": "9tt34CM6P-gr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "# sample_img_batch, sample_cap_batch = next(iter(ds_train))\n",
    "# # plt.imshow(sample_img_batch)\n",
    "# tf.shape(sample_img_batch)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# sample_cap_batch\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# classes = ds_info.features['objects']['label'].names\n",
    "# num_classes = len(classes)\n",
    "# print(num_classes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "from tensorflow.keras import backend as K\n",
    "import itertools\n",
    "\n",
    "def w_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    tf.print(\"--Snehal---------------------------------------------------------------------->\")\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = K.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = K.max(y_pred, axis=1)\n",
    "    y_pred_max = K.expand_dims(y_pred_max, 1)\n",
    "    y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "    for c_p, c_t in itertools.product(range(nb_cl), range(nb_cl)):\n",
    "        final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\n",
    "        \n",
    "    tf.print(\"Snehal\")\n",
    "    tf.print(tf.shape(y_pred))\n",
    "    tf.print(tf.shape(y_true))\n",
    "    tf.print(tf.shape(final_mask))\n",
    "    return K.categorical_crossentropy(y_pred, y_true) * final_mask\n",
    "\n",
    "w_array = np.ones((2,2))\n",
    "w_array[1,0] = 1.2\n",
    "ncce = functools.partial(w_categorical_crossentropy, weights=w_array)\n",
    "ncce.__name__ = \"w_categorical_crossentropy\"\n",
    "# ncce.__module__ = my_other_view.__module__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "#Save the model after every epoch.\n",
    "# mc_top = ModelCheckpoint(top_layers_checkpoint_path, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#Save the TensorBoard logs. histogram_freq was 1 (gave errors) and now is 0. write_images was True (read that this is heavy) and now is False\n",
    "# tb = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=False, write_images=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "N_LABELS = 80\n",
    "\n",
    "# write code here to get the input of the image_model\n",
    "new_input = image_model.input \n",
    "# write code here to get the output of the image_model\n",
    "hidden_layer = image_model.output \n",
    "\n",
    "#build the final model using both input & output layer\n",
    "image_features_extract_model = tf.keras.Model(inputs=new_input, outputs=hidden_layer)\n",
    "\n",
    "x = image_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = tf.keras.layers.Dense(80, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = tf.keras.Model(inputs=image_model.input, outputs=predictions)\n",
    "\n",
    "for layer in image_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss=ncce, metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "\n",
    "callbacks = [\n",
    "    TensorBoard(),\n",
    "    ModelCheckpoint('checkpoints/')\n",
    "]\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_test, epochs=12, callbacks=callbacks)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/12\n",
      "    18/118287 [..............................] - ETA: 42:22:36 - loss: 161.9847 - accuracy: 0.1472"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from kerod.core.standard_fields import BoxField\n",
    "# from kerod.core.learning_rate_schedule import LearningRateScheduler\n",
    "# from kerod.model import factory\n",
    "\n",
    "# from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "# # Number of classes of COCO\n",
    "# classes = ds_info.features['objects']['label'].names\n",
    "# num_classes = len(classes)\n",
    "\n",
    "# model_faster_rcnn = factory.build_model(num_classes)\n",
    "# base_lr = 0.02\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr, momentum=0.9)\n",
    "# model_faster_rcnn.compile(optimizer=optimizer, loss=None)\n",
    "\n",
    "# #The numbering of epochs (LearningRateScheduler) starts at 0.\n",
    "# # Which means the decrease will happens on the epoch 9:\n",
    "# #(8 + 1: numbering of fit logging starts at 1)\n",
    "# callbacks = [\n",
    "#     TensorBoard(),\n",
    "#     ModelCheckpoint('checkpoints/')\n",
    "# ]\n",
    "\n",
    "# model_faster_rcnn.fit(ds_train, validation_data=ds_test, epochs=12, callbacks=callbacks)"
   ],
   "outputs": [],
   "metadata": {
    "id": "r4o3t4PCLagO",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the weights for the serving\n",
    "model.save_weights('final_weights.h5')"
   ],
   "outputs": [],
   "metadata": {
    "id": "LdQtj-HGlNxg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export a saved model for serving purposes\n",
    "model.export_for_serving('serving')"
   ],
   "outputs": [],
   "metadata": {
    "id": "S-g9yIf8lNxh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation of few images"
   ],
   "metadata": {
    "id": "_ZrcN9snlNxh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from od.utils.drawing import BoxDrawer\n",
    "\n",
    "drawer = BoxDrawer(classes)\n",
    "\n",
    "for i, example in enumerate(ds_val):\n",
    "    inputs, ground_truths = example\n",
    "    out = model.predict_on_batch(inputs)\n",
    "    boxes, scores, labels, valid_detections = out\n",
    "    # Will draw the results\n",
    "    drawer(\n",
    "        inputs['images'],\n",
    "        boxes,\n",
    "        scores=scores,\n",
    "        labels=labels,\n",
    "        num_valid_detections=valid_detections\n",
    "    )\n",
    "    if i == 5:\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {
    "id": "LXhTDloWlNxi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorboard"
   ],
   "metadata": {
    "id": "CD14aaUMEudZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load TENSORBOARD\n",
    "%load_ext tensorboard\n",
    "# Start TENSORBOARD\n",
    "%tensorboard --logdir logs"
   ],
   "outputs": [],
   "metadata": {
    "id": "Ec4-mdjcR_wy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Coco evaluation\n"
   ],
   "metadata": {
    "id": "ItPs2V7tlNxj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the dataset"
   ],
   "metadata": {
    "id": "cx9Jkvs2lNxj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds_val, ds_info = tfds.load(name=\"coco/2017\", split=\"validation\", shuffle_files=False, with_info=True)\n",
    "# category_ids basicaly map the index 0 the id\n",
    "# e.g: 0 -> 1, 2 -> 3, 79 -> 90\n",
    "category_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]"
   ],
   "outputs": [],
   "metadata": {
    "id": "hPAIjroqlNxj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Super dirty but the evaluation works"
   ],
   "metadata": {
    "id": "kcnsF4UjlNxj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. perform the analysis"
   ],
   "metadata": {
    "id": "KSqnoi3jlNxk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from kerod.core.standard_fields import DatasetField, BoxField             \n",
    "from kerod.core.box_ops import convert_to_center_coordinates              \n",
    "from kerod.dataset.preprocessing import resize_to_min_dim                 \n",
    "                                                                       \n",
    "results = []                                                           \n",
    "                                                                       \n",
    "for i, example in enumerate(ds_val): \n",
    "    print(i)\n",
    "    # preprocess image\n",
    "    image = example['image'][:, :, ::-1]\n",
    "    image = resize_to_min_dim(image, 800.0, 1333.0)         \n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32) \n",
    "    inputs = {\n",
    "        'images': tf.expand_dims(image, axis=0),\n",
    "        'images_information':tf.expand_dims(image_information, axis=0)\n",
    "    }\n",
    "                                                                 \n",
    "    # predict                                                          \n",
    "    boxes, scores, labels, valid_detections = model_faster_rcnn.predict_on_batch(inputs)\n",
    "                                                                       \n",
    "    # Post processing and append to coco results                       \n",
    "    bbox = boxes[0] * tf.tile(\n",
    "        tf.expand_dims(tf.cast(example['image'].shape[:2], tf.float32), axis=0),\n",
    "        [1, 2])                   \n",
    "    scores = scores[0]                                           \n",
    "    labels = labels[0]                                           \n",
    "    for i in range(valid_detections[0]):\n",
    "        # Convert from [y_min, x_min, y_max, x_max] to coco format [x_min, y_min, w, h]\n",
    "        sbox = bbox[i].numpy()\n",
    "        sbox = [sbox[1], sbox[0], sbox[3] - sbox[1], sbox[2] - sbox[0]]\n",
    "        res = {                                                        \n",
    "                'image_id': int(example['image/id']),                       \n",
    "                'category_id': category_ids[int(labels[i])],           \n",
    "                'bbox': [round(float(c), 4) for c in sbox],\n",
    "                'score': round(float(scores[i]), 4),                     \n",
    "            }                                                          \n",
    "        results.append(res)                                            \n",
    "                                                   \n",
    "                                                                       \n",
    "with open('coco_results.json', 'w') as f:                              \n",
    "    json.dump(results, f)"
   ],
   "outputs": [],
   "metadata": {
    "id": "8EBgTruQlNxk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. install the coco library to compute the performances"
   ],
   "metadata": {
    "id": "fxhTvq_UlNxl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip\n",
    "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ],
   "outputs": [],
   "metadata": {
    "id": "iE3QzM5xlNxl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. compute the performances"
   ],
   "metadata": {
    "id": "Bpw4HGtplNxm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "with open('coco_results_corrected.json', 'r') as f:                              \n",
    "    results = json.load(f)\n",
    "coco = COCO('./annotations/instances_val2017.json')\n",
    "ret = {}\n",
    "\n",
    "cocoDt = coco.loadRes(results)\n",
    "cocoEval = COCOeval(coco, cocoDt, 'bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ],
   "outputs": [],
   "metadata": {
    "id": "B2q-THNvlNxm"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coco_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py373",
   "language": "python",
   "name": "py373"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}