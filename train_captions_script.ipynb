{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gd0q3ftWEVR7"
   },
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Model\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import socket\n",
    "import pickle5 as pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constants import test, BoxField, DatasetField\n",
    "from common.config import Config\n",
    "from common.logger import Logger\n",
    "from common.communication import Client\n",
    "from common.communication import Server\n",
    "from common.helper import ImagesInfo \n",
    "from common.timekeeper import TimeKeeper\n",
    "from common.helper import read_image, filt_text, get_predictions,process_predictions\n",
    "from CaptionModel import CaptionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_image_height = 150\n",
    "h_image_height = 200\n",
    "# max_tokenized_words = 5000\n",
    "max_tokenized_words = 5000\n",
    "# test_files_num = 118287\n",
    "test_files_num = 10000\n",
    "\n",
    "def get_reshape_size(image_height):\n",
    "    if(image_height == 100):\n",
    "        return 1\n",
    "    if(image_height == 125):\n",
    "        return 2\n",
    "    if(image_height == 150):\n",
    "        return 3\n",
    "    if(image_height >= 175) and (image_height <= 200) :\n",
    "        return 4\n",
    "    if(image_height == 225) :\n",
    "        return 5\n",
    "    if(image_height == 250) :\n",
    "        return 6\n",
    "    if(image_height == 275) :\n",
    "        return 7\n",
    "    if(image_height == 299):\n",
    "        return 8\n",
    "    if(image_height == 300):\n",
    "        return 8\n",
    "    if(image_height == 325):\n",
    "        return 8\n",
    "    if(image_height == 350):\n",
    "        return 9\n",
    "    if(image_height == 375):\n",
    "        return 10\n",
    "    if(image_height == 400):\n",
    "        return 11\n",
    "    else:\n",
    "        return 8\n",
    "h_cap_feature_size = get_reshape_size(h_image_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In NimbleBox\n"
     ]
    }
   ],
   "source": [
    "in_nimble = False\n",
    "in_WSL = False\n",
    "in_gcp = False\n",
    "\n",
    "host = socket.gethostname()\n",
    "if('cuda' in host):\n",
    "    in_nimble = True\n",
    "    print(\"In NimbleBox\")\n",
    "if(host == 'LTsuphale-NC2JM'):\n",
    "    in_WSL = True\n",
    "    print(\"In WSL\")\n",
    "if(host == 'instance-1'):\n",
    "    in_gcp = True\n",
    "    print(\"In GCP\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jPM7st2EVSF",
    "outputId": "d2b7af7c-3f37-435f-e5cf-b8f96b372a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total images present in the images folder: 118287\n"
     ]
    }
   ],
   "source": [
    "images_path = ''\n",
    "text_file = ''\n",
    "\n",
    "\n",
    "if(in_WSL == True):\n",
    "    images_path='/home/suphale/snehal_bucket/coco/raw-data/train2017/'\n",
    "    text_file = '/home/suphale/snehal_bucket/coco/raw-data/train2017/captions_' + str(test_files_num) +'.txt'\n",
    "    list_file = '/home/suphale/snehal_bucket/coco/raw-data/train2017/images_' + str(test_files_num) +'.txt'\n",
    "if(in_nimble == True):\n",
    "    images_path='/mnt/disks/user/project/coco/train2017/'\n",
    "    text_file = '/mnt/disks/user/project/WorkSpace/lists/captions_' + str(test_files_num) +'.txt'\n",
    "    list_file = '/mnt/disks/user/project/WorkSpace/lists/images_' + str(test_files_num) +'.txt'\n",
    "if(in_gcp == True):\n",
    "    images_path='/home/suphale/coco/train2017/'\n",
    "    text_file = '/home/suphale/WorkSpace/lists/captions_' + str(test_files_num) +'.txt'\n",
    "    list_file = '/home/suphale/WorkSpace/lists/images_' + str(test_files_num) +'.txt'\n",
    "\n",
    "# Find out total number of images in the images folder\n",
    "all_imgs_folder = glob.glob(images_path + '/*.jpg',recursive=True)\n",
    "all_imgs_folder = sorted(all_imgs_folder)\n",
    "total_num_images_folder = len(all_imgs_folder)\n",
    "print(\"The total images present in the images folder: {}\".format(total_num_images_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total images present in the dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "all_imgs = [line.rstrip() for line in open(list_file)]\n",
    "all_imgs = sorted(all_imgs)\n",
    "total_num_images = len(all_imgs)\n",
    "print(\"The total images present in the dataset: {}\".format(total_num_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FinyQD3kEVSJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total images present in the dataset: 10000\n",
      "The total number of lines in the caption file: 50229\n"
     ]
    }
   ],
   "source": [
    "#Visualise both the images & text present in the dataset\n",
    "print(\"The total images present in the dataset: {}\".format(total_num_images))\n",
    "num_lines = sum(1 for line in open(text_file))\n",
    "print(\"The total number of lines in the caption file: {}\".format(num_lines))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bzryG2erEVSJ"
   },
   "outputs": [],
   "source": [
    "# define a function to clean text data\n",
    "def extract_jpg_caption(line):\n",
    "    char_filter = r\"[^\\w]\"\n",
    "\n",
    "    jpg_path = None\n",
    "    caption = None\n",
    "\n",
    "    jpg_position = line.find(\".jpg\")\n",
    "    if(jpg_position != -1):\n",
    "        jpg_path = images_path + '/' + line[:jpg_position+4]\n",
    "\n",
    "        caption = line[jpg_position+5:].strip()\n",
    "\n",
    "        # convert words to lower case\n",
    "        caption = caption.lower()\n",
    "\n",
    "        # split into words\n",
    "        words = caption.split()\n",
    "\n",
    "        # strip whitespace from all words\n",
    "        words = [word.strip() for word in words]\n",
    "\n",
    "        # join back words to get document\n",
    "        caption = \" \".join(words)\n",
    "\n",
    "        # remove unwanted characters\n",
    "        caption = re.sub(char_filter, \" \", caption)\n",
    "\n",
    "        # remove unwanted characters\n",
    "        caption = re.sub(r\"\\.\", \" \", caption)\n",
    "\n",
    "        # replace multiple whitespaces with single whitespace\n",
    "        caption = re.sub(r\"\\s+\", \" \", caption)\n",
    "\n",
    "        # strip whitespace from document\n",
    "        caption = caption.strip()\n",
    "\n",
    "        caption = '<start> ' + caption + ' <end>'\n",
    "\n",
    "    return jpg_path, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nGF8eqJYEVSK"
   },
   "outputs": [],
   "source": [
    "#store all the image id here\n",
    "all_img_id= [] \n",
    "#store all the image path here\n",
    "all_img_vector=[]\n",
    "#store all the captions here\n",
    "annotations= [] \n",
    "# list of all captions in word list format\n",
    "annotations_word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GwWfE7lMEVSK"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    #your code here\n",
    "    file  = open(filename, 'r') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    text = \"\"\n",
    "    count = 0\n",
    "    for line in Lines:\n",
    "        jpg_path, caption = extract_jpg_caption(line)\n",
    "        if(jpg_path != None):\n",
    "            all_img_id.append(count)\n",
    "            all_img_vector.append(jpg_path)\n",
    "            annotations.append(caption)\n",
    "            word_list = caption.split()\n",
    "            annotations_word_list.append(word_list)\n",
    "            text += \" \" + caption\n",
    "            count += 1\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8hQZDjOEVSL",
    "outputId": "67aee2a4-af6c-4412-8a94-dbc81cdeb146"
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the text file into a seperate variable\n",
    "doc = load_doc(text_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wugOWpmjEVSM",
    "outputId": "fb87beaa-4af0-44a7-c5e7-86a1353b37f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions present in the dataset: 50036\n",
      "Total images present in the dataset: 50036\n"
     ]
    }
   ],
   "source": [
    "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
    "print(\"Total images present in the dataset: \" + str(len(all_img_vector)))\n",
    "total_images = len(all_img_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "oqVoDYnfEVSN",
    "outputId": "242cfc4d-6933-4ede-9df8-37b815dcfa0f"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dkhVhIP4EVSP"
   },
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "\n",
    "# your code here\n",
    "# max_tokenized_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_tokenized_words+1,oov_token='<unknown>')\n",
    "tokenizer.fit_on_texts(annotations_word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nX8PDZ3_EVSP",
    "outputId": "e538e663-d4de-42c3-fa3c-364a2c5755f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9464\n"
     ]
    }
   ],
   "source": [
    "# Create word-to-index and index-to-word mappings.\n",
    "VOCABULARY_SIZE = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(VOCABULARY_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "xCRGbE_fEVSP",
    "outputId": "27323304-5b68-4cd4-9606-f3cf54f744be"
   },
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions ^ store it to a vairable\n",
    "\n",
    "MAX_SEQ_LENGTH = 25\n",
    "\n",
    "cap_vector = annotations_word_list\n",
    "Y_encoded = tokenizer.texts_to_sequences(cap_vector)\n",
    "cap_vector_encoded_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QZ0EAZKLEVSU"
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "batch_size = 32\n",
    "def read_image(image_path,label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image_height = tf.shape(image)[0]\n",
    "    image = tf.image.resize(image, (h_image_height, h_image_height))\n",
    "#     image = tf.image.resize(image, (299, 299))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GkQryLPEVSQ",
    "outputId": "57d5c552-7bc7-4ad1-9043-bcc79056c467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images for training=39628\n",
      "Total Images for testing (validation)=9907\n",
      "Total Images for final random testing=501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train, validation and final test sets\n",
    "img_train, final_img_test, cap_train, final_cap_test = train_test_split(all_img_vector, cap_vector_encoded_padded, test_size=0.01, random_state=42)\n",
    "img_train, img_test, cap_train, cap_test = train_test_split(img_train, cap_train, test_size=0.2, random_state=42)\n",
    "\n",
    "total_training_images = len(img_train)\n",
    "total_test_images = len(img_test)\n",
    "total_final_testing_images = len(final_img_test)\n",
    "print(\"Total Images for training=%d\" % (total_training_images))\n",
    "print(\"Total Images for testing (validation)=%d\" % (total_test_images))\n",
    "print(\"Total Images for final random testing=%d\" % (total_final_testing_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "S6rCEjr3EVSR"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((img_train, cap_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((img_test, cap_test))\n",
    "final_test_dataset = tf.data.Dataset.from_tensor_slices((final_img_test, final_cap_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "C5sEcEB8EVSW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function read_image at 0x7f61824cc940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function read_image at 0x7f61824cc940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "#write your code here\n",
    "train_dataset = train_dataset.map(read_image)\n",
    "train_dataset.shuffle(buffer_size=1024,reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(read_image)\n",
    "test_dataset.shuffle(buffer_size=1024,reshuffle_each_iteration=True)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj2ntOu6jHqU"
   },
   "source": [
    "A utility function to display image retrieved from a dataset and its corresponding caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hKTzT0RFEVSY"
   },
   "outputs": [],
   "source": [
    "# A utility function to display image retrieved from a dataset and its corresponding caption\n",
    "def show_image_caption_from_dataset(image,label):\n",
    "    plt.imshow(image)\n",
    "    for x in label.numpy():\n",
    "        if(x != 0):\n",
    "            print(tokenizer.index_word[x], end =\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oNZnAGSsEVSb"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "# write code here to get the input of the image_model\n",
    "new_input = image_model.input \n",
    "# write code here to get the output of the image_model\n",
    "hidden_layer = image_model.output \n",
    "\n",
    "#build the final model using both input & output layer\n",
    "image_features_extract_model = keras.Model(inputs=new_input, outputs=hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K34JkKjtEVSb"
   },
   "outputs": [],
   "source": [
    "# write your code to extract features from each image in the dataset\n",
    "def extract_image_features(sample_img_batch):\n",
    "    features = image_features_extract_model(sample_img_batch)\n",
    "    features = tf.reshape(features, [sample_img_batch.shape[0],h_cap_feature_size*h_cap_feature_size, 2048])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmmaxLLREVSe",
    "outputId": "78e141c8-4b94-4665-ec1e-50c5357eb2c6"
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "sample_img_batch = extract_image_features(sample_img_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vztEm1vrEVSe",
    "outputId": "52263984-47c4-47bf-9683-614f29ef397a"
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(test_dataset))\n",
    "sample_img_batch = extract_image_features(sample_img_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DrdrgKbTEVSf"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256 \n",
    "units = 512\n",
    "vocab_size = max_tokenized_words + 1\n",
    "train_num_steps = total_training_images //batch_size #len(total train images) // BATCH_SIZE\n",
    "test_num_steps = total_test_images //batch_size #len(total test images) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rq0rRVQEVSg"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_pgG7OkBEVSg"
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # build your Dense layer with relu activation\n",
    "#         self.pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation='relu')\n",
    "        \n",
    "    def call(self, features):\n",
    "        # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "#         features = tf.expand_dims(features, 0) \n",
    "#         features = self.pooling(features)\n",
    "        features = self.dense(features)\n",
    "        return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "mnNo7xm7EVSg"
   },
   "outputs": [],
   "source": [
    "encoder=Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5d6VuSdEVSg"
   },
   "source": [
    "### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "D02WLWNNEVSg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import tanh\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.units=units\n",
    "        # build your Dense layer\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        # build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # build your final Dense layer with unit 1\n",
    "        # self.V = tf.keras.layers.Dense(1, activation='softmax')\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features shape: (batch_size, 8*8, embedding_dim)\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        score = tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        score = self.V(score)\n",
    "        attention_weights = softmax(score, axis=1)\n",
    "\n",
    "        # shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        # reduce the shape to (batch_size, embedding_dim)\n",
    "        # context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.reduce_mean(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITGBKawcEVSh"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yP8k2UdbEVSh"
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        # iniitalise your Attention model with units\n",
    "        self.attention = Attention_model(self.units)\n",
    "        # build your Embedding layer\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        # build your Dense layer\n",
    "        self.d1 = tf.keras.layers.Dense(self.units)\n",
    "        # build your Dense layer\n",
    "        self.d2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        #create your context vector & attention weights from attention model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed = self.embed(x)\n",
    "        # Concatenate your input with the context vector from attention layer. \n",
    "        # Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)\n",
    "        # Extract the output & hidden state from GRU layer. \n",
    "        # Output shape : (batch_size, max_length, hidden_size)\n",
    "        output, state = self.gru(embed)\n",
    "        output = self.d1(output)\n",
    "        # shape : (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) \n",
    "        # shape : (batch_size * max_length, vocab_size)\n",
    "        output = self.d2(output)\n",
    "\n",
    "        return output, state, attention_weights\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JHU0wjNaEVSh"
   },
   "outputs": [],
   "source": [
    "decoder=Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCfPNkjSEVSh",
    "outputId": "81a75bd3-ad2a-4665-e9cb-19ef4d186514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape from Encoder: (32, 16, 256)\n",
      "Predcitions shape from Decoder: (32, 5001)\n",
      "Attention weights shape from Decoder: (32, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "features=encoder(sample_img_batch)\n",
    "\n",
    "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
    "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
    "\n",
    "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
    "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
    "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
    "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fQSaRpQQEVSi"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "S8zsYc0_EVSi"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-zxPDixbEVSj"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./ckpt\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GppbW39tEVSk"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "YDVWr-jjEVSl"
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    img_tensor = extract_image_features(img_tensor)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the fixed length vector of Encoder-Decoder model\n",
    "        features = encoder(img_tensor)\n",
    "        # Loop through for max_length times\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Get predictions from the decoder\n",
    "            #print(\"dec_input=%s features=%s hidden=%s\" % (str(type(dec_input)), str(type(features)), str(type(hidden))))\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Get the next target vector as dec_input\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "0nDn-xmOEVSn"
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def test_step(img_tensor, target):\n",
    "    #write your code here to do the testing steps\n",
    "    loss = 0\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    img_tensor = extract_image_features(img_tensor)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    features = encoder(img_tensor)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "        loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "        dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "T2s6DLVNEVSn"
   },
   "outputs": [],
   "source": [
    "def test_loss_cal(test_dataset):\n",
    "    total_loss = 0\n",
    "\n",
    "    #write your code to get the average loss result on your test data\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n",
    "        batch_loss, t_loss = test_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_test_loss=total_loss / test_num_steps\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iw_rObMKEVSn",
    "outputId": "e3dc3b43-65e9-49d0-ea03-055dac1ac4a3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "loss_plot = []\n",
    "test_loss_plot = []\n",
    "EPOCHS = 5\n",
    "\n",
    "best_test_loss=100\n",
    "for epoch in tqdm(range(0, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_train_loss=total_loss / train_num_steps\n",
    "        \n",
    "    loss_plot.append(avg_train_loss)    \n",
    "\n",
    "    test_loss = test_loss_cal(test_dataset)\n",
    "    test_loss_plot.append(test_loss)\n",
    "    \n",
    "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
    "        best_test_loss = test_loss\n",
    "        ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "50gx5tp5EVSo",
    "outputId": "6c6a9b99-be0e-4e54-e152-46fc8b8875cb"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot,label=\"loss\")\n",
    "plt.plot(test_loss_plot,label=\"test_loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTpgLUbTEVSp"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_features_shape = h_cap_feature_size * h_cap_feature_size\n",
    "    attention_plot = np.zeros((MAX_SEQ_LENGTH, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    # process the input image to desired format before extracting features\n",
    "    temp_input = tf.expand_dims(read_image(image,[])[0], 0) \n",
    "    # Extract features using our feature extraction model\n",
    "    img_tensor_val = extract_image_features(temp_input)\n",
    "    # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    # extract the features by passing the input to encoder\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(MAX_SEQ_LENGTH):\n",
    "        # get the output from decoder\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        #extract the predicted id(embedded value) which carries the max value\n",
    "        predicted_id = tf.argmax(tf.transpose(predictions))\n",
    "        predicted_id = predicted_id.numpy()[0]\n",
    "        # map the id to the word from tokenizer and append the value to the result list\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot,predictions\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot,predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSMhRhbIEVSp"
   },
   "source": [
    "### Beam Search(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DTtqauGEVSp"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate(image, beam_index = 0): #your value for beam index):\n",
    "\n",
    "    final_caption = \"ToDo\"\n",
    "    #write your code to evaluate the result using beam search\n",
    "                  \n",
    "    return final_caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNm6XDyhEVSq"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def plot_attmap(caption, weights, image):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    temp_img = np.array(Image.open(image))\n",
    "    \n",
    "    len_cap = len(caption)\n",
    "    for cap in range(len_cap):\n",
    "        weights_img = np.reshape(weights[cap], (h_cap_feature_size,h_cap_feature_size))\n",
    "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
    "        \n",
    "        ax = fig.add_subplot((len_cap//2)+1, (len_cap//2)+1, cap+1)\n",
    "        ax.set_title(caption[cap], fontsize=15)\n",
    "        \n",
    "        img=ax.imshow(temp_img)\n",
    "        \n",
    "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
    "        ax.axis('off')\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLXHjJmGEVSq"
   },
   "outputs": [],
   "source": [
    "def filt_text(text):\n",
    "    filt=['<start>','<unk>','<end>'] \n",
    "    temp= text.split()\n",
    "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
    "    text=' '.join(temp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "nUEIaCdXEVSq",
    "outputId": "2363d7ee-7c53-420b-e7ce-688d7f8377aa"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "rid = np.random.randint(0, total_final_testing_images)\n",
    "test_image = final_img_test[rid]\n",
    "# test_image = './Images/413231421_43833a11f5.jpg'\n",
    "# test_image = '/content/drive/MyDrive/TestImages/3637013_c675de7705.jpg'\n",
    "\n",
    "real_caption = final_cap_test[rid]\n",
    "# real_caption = '<start> A couple stands close at the water edge <end>'\n",
    "# real_caption = '<start> black dog is digging in the snow <end>'\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in final_cap_test[rid] if i not in [0]])\n",
    "\n",
    "t0= time.perf_counter()\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "t1 = time.perf_counter() - t0\n",
    "print(\"Time elapsed: \", t1)\n",
    "\n",
    "real_caption=filt_text(real_caption)      \n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "print(f\"BLEU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', pred_caption)\n",
    "plot_attmap(result, attention_plot, test_image)\n",
    "\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4ZUTC3xEVSr"
   },
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# saving\n",
    "temp_dir = 'temp/i_%d' % (h_image_height)\n",
    "try: \n",
    "    os.mkdir(temp_dir) \n",
    "except OSError as error: \n",
    "    print(error)  \n",
    "with open(temp_dir + '/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#decoder.save('MyModel',save_format='tf')\n",
    "model_json = image_features_extract_model.to_json()\n",
    "with open(temp_dir + \"/image_features_extract_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "image_features_extract_model.save_weights(temp_dir + \"/image_features_extract_model.h5\")\n",
    "print(\"Saved image_features_extract_model to disk\")\n",
    "\n",
    "decoder.save_weights(temp_dir + \"/decoder.h5\")\n",
    "encoder.save_weights(temp_dir + \"/encoder.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the model from JSON file\n",
    "with open(temp_dir + '/image_features_extract_model.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "#load the model architecture \n",
    "model_j = tf.keras.models.model_from_json(json_savedModel)\n",
    "model_j.load_weights(temp_dir + '/image_features_extract_model.h5')\n",
    "\n",
    "decoder.load_weights(temp_dir + \"/decoder.h5\")\n",
    "encoder.load_weights(temp_dir + \"/encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "test_image = './test_images/000000001924.jpg'\n",
    "# test_image = '/content/drive/MyDrive/TestImages/3637013_c675de7705.jpg'\n",
    "\n",
    "# real_caption = final_cap_test[rid]\n",
    "real_caption = '<start> A couple stands close at the water edge <end>'\n",
    "# real_caption = '<start> black dog is digging in the snow <end>'\n",
    "# real_caption = ' '.join([tokenizer.index_word[i] for i in final_cap_test[rid] if i not in [0]])\n",
    "\n",
    "t0= time.perf_counter()\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "t1 = time.perf_counter() - t0\n",
    "print(\"Time elapsed: \", t1)\n",
    "\n",
    "real_caption=filt_text(real_caption)      \n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "print(f\"BLEU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', pred_caption)\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cap_01.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "ML and DL with CUDA 10.2",
   "language": "python",
   "name": "cuda101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
