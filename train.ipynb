{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install --upgrade git+https://github.com/EmGarr/kerod.git"
   ],
   "outputs": [],
   "metadata": {
    "id": "1jVKNjhdLFUQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU device not found\n"
     ]
    }
   ],
   "metadata": {
    "id": "CET-72i5EmKn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import  functools\n",
    "import  tensorflow as tf\n",
    "import  tensorflow_datasets as tfds\n",
    "from    tensorflow.keras.utils import to_categorical\n",
    "import  matplotlib.pyplot as plt\n",
    "from    tensorflow.keras import layers\n",
    "from    tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from    tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from Helper import Config, ImagesInfo, Logger, Client, TimeKeeper\n",
    "from Helper import read_image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "data_dir='/home/suphale/coco'\n",
    "N_LABELS = 80\n",
    "N_EPOCHS = 1\n",
    "TRAIN_MODE = False\n",
    "# split_train = \"train\"\n",
    "# split_val = \"validation\"\n",
    "split_train = \"train[:1%]\"\n",
    "split_val = \"validation[:1%]\"\n",
    "h_image_height = 299\n",
    "h_image_width = 299\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tk = TimeKeeper()\n",
    "cfg = Config()\n",
    "client = Client(cfg)\n",
    "imagesInfo = ImagesInfo(cfg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "In WSL\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class BoxField:\n",
    "    BOXES = 'bbox'\n",
    "    KEYPOINTS = 'keypoints'\n",
    "    LABELS = 'label'\n",
    "    MASKS = 'masks'\n",
    "    NUM_BOXES = 'num_boxes'\n",
    "    SCORES = 'scores'\n",
    "    WEIGHTS = 'weights'\n",
    "\n",
    "class DatasetField:\n",
    "    IMAGES = 'images'\n",
    "    IMAGES_INFO = 'images_information'\n",
    "    IMAGES_PMASK = 'images_padding_mask'\n",
    "\n",
    "def my_preprocess(inputs):\n",
    "    image = inputs['image']\n",
    "    image = tf.image.resize(image, (h_image_height, h_image_width))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "\n",
    "    targets = inputs['objects']\n",
    "\n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    inputs = {DatasetField.IMAGES: image, DatasetField.IMAGES_INFO: image_information}\n",
    "\n",
    "    # ground_truths = {\n",
    "    #     BoxField.BOXES: targets[BoxField.BOXES] * tf.tile(image_information[tf.newaxis], [1, 2]),\n",
    "    #     BoxField.LABELS: tf.cast(targets[BoxField.LABELS], tf.int32),\n",
    "    #     BoxField.NUM_BOXES: tf.shape(targets[BoxField.LABELS]),\n",
    "    #     BoxField.WEIGHTS: tf.fill(tf.shape(targets[BoxField.LABELS]), 1.0)\n",
    "    # }\n",
    "    ground_truths = tf.cast(targets[BoxField.LABELS], tf.int32)\n",
    "    ground_truths = tf.one_hot(ground_truths, depth=N_LABELS, dtype=tf.int32)\n",
    "    ground_truths = tf.reduce_sum(ground_truths, 0)\n",
    "    ground_truths = tf.greater( ground_truths, tf.constant( 0 ) )    \n",
    "    ground_truths = tf.where (ground_truths, 1, 0) \n",
    "    return image, ground_truths\n",
    "\n",
    "def expand_dims_for_single_batch(image, ground_truths):\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    ground_truths = tf.expand_dims(ground_truths, axis=0)\n",
    "    return image, ground_truths\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "ds_train, ds_info = tfds.load(name=\"coco/2017\", split=split_train, data_dir=data_dir, shuffle_files=True, download=False, with_info=True)\n",
    "ds_train = ds_train.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_val = tfds.load(name=\"coco/2017\", split=split_val, data_dir=data_dir, shuffle_files=True, download=False)\n",
    "ds_val = ds_val.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7ff36d5e44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7ff36d5e44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function my_preprocess at 0x7ff36d5e44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "metadata": {
    "id": "1cC2k8osNGFw",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and train the network\n"
   ],
   "metadata": {
    "id": "9tt34CM6P-gr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Find total number of classes in the coco dataset\n",
    "classes = ds_info.features['objects']['label'].names\n",
    "num_classes = len(classes)\n",
    "print(num_classes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "x = image_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "predictions = tf.keras.layers.Dense(80, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=image_model.input, outputs=predictions)\n",
    "\n",
    "for layer in image_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss=ncce, metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for layer in model.layers:\n",
    "    if(layer.trainable == True):\n",
    "        print(layer.trainable)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "callbacks = [\n",
    "    TensorBoard(),\n",
    "    ModelCheckpoint('checkpoints/')\n",
    "]\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=N_EPOCHS, callbacks=callbacks)\n",
    "# Save the weights for the serving\n",
    "model.save_weights(cfg.temp_path + '/coco_classification_weights.h5')\n",
    "model.save(cfg.temp_path + '/model')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation of few images"
   ],
   "metadata": {
    "id": "_ZrcN9snlNxh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "for test_index in range(10):\n",
    "    sample_img_batch, sample_cap_batch = next(iter(ds_val))\n",
    "    s = model(sample_img_batch)\n",
    "    # plt.imshow(tf.squeeze(sample_img_batch, [0]))\n",
    "\n",
    "    print(\"Reference  : \", end=' ')\n",
    "    n = sample_cap_batch.numpy()\n",
    "    index = 0\n",
    "    for x in n[0]:\n",
    "        if x > 0.1:\n",
    "            print(\"%s,\" % (classes[index]), end=' ')\n",
    "        index += 1\n",
    "    print(\"\")\n",
    "    print(\"Prediction : \", end=' ')\n",
    "    n = s.numpy()\n",
    "    index = 0\n",
    "    for x in n[0]:\n",
    "        if x > 0.5:\n",
    "            print(\"%s(%.2f),\" % (classes[index],x), end=' ')\n",
    "        index += 1\n",
    "    print(\"\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reference:\n",
      "remote, book, \n",
      "Prediction:\n",
      "bottle(0.54), \n",
      "Reference:\n",
      "person, bus, book, \n",
      "Prediction:\n",
      "bus(0.99), \n",
      "Reference:\n",
      "person, tennis racket, \n",
      "Prediction:\n",
      "person(0.99), sports ball(0.52), tennis racket(1.00), \n",
      "Reference:\n",
      "person, tennis racket, \n",
      "Prediction:\n",
      "person(0.99), sports ball(0.52), tennis racket(1.00), \n",
      "Reference:\n",
      "person, bus, book, \n",
      "Prediction:\n",
      "bus(0.99), \n",
      "Reference:\n",
      "person, car, handbag, \n",
      "Prediction:\n",
      "person(0.96), skateboard(0.89), \n",
      "Reference:\n",
      "person, tennis racket, \n",
      "Prediction:\n",
      "person(0.99), sports ball(0.52), tennis racket(1.00), \n",
      "Reference:\n",
      "person, tennis racket, \n",
      "Prediction:\n",
      "person(0.99), sports ball(0.52), tennis racket(1.00), \n",
      "Reference:\n",
      "person, car, handbag, \n",
      "Prediction:\n",
      "person(0.96), skateboard(0.89), \n",
      "Reference:\n",
      "remote, book, \n",
      "Prediction:\n",
      "bottle(0.54), \n"
     ]
    }
   ],
   "metadata": {
    "id": "LXhTDloWlNxi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorboard"
   ],
   "metadata": {
    "id": "CD14aaUMEudZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Load TENSORBOARD\n",
    "%load_ext tensorboard\n",
    "# Start TENSORBOARD\n",
    "%tensorboard --logdir logs"
   ],
   "outputs": [],
   "metadata": {
    "id": "Ec4-mdjcR_wy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# ds_info"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='coco',\n",
       "    full_name='coco/2017/1.1.0',\n",
       "    description=\"\"\"\n",
       "    COCO is a large-scale object detection, segmentation, and\n",
       "    captioning dataset.\n",
       "    \n",
       "    Note:\n",
       "     * Some images from the train and validation sets don't have annotations.\n",
       "     * Coco 2014 and 2017 uses the same images, but different train/val/test splits\n",
       "     * The test split don't have any annotations (only images).\n",
       "     * Coco defines 91 classes but the data only uses 80 classes.\n",
       "     * Panotptic annotations defines defines 200 classes but only uses 133.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    \n",
       "    This version contains images, bounding boxes and labels for the 2017 version.\n",
       "    \n",
       "    \"\"\",\n",
       "    homepage='http://cocodataset.org/#home',\n",
       "    data_path='/home/suphale/coco/coco/2017/1.1.0',\n",
       "    download_size=25.20 GiB,\n",
       "    dataset_size=24.98 GiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "        'image/filename': Text(shape=(), dtype=tf.string),\n",
       "        'image/id': tf.int64,\n",
       "        'objects': Sequence({\n",
       "            'area': tf.int64,\n",
       "            'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
       "            'id': tf.int64,\n",
       "            'is_crowd': tf.bool,\n",
       "            'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80),\n",
       "        }),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=40670, num_shards=64>,\n",
       "        'train': <SplitInfo num_examples=118287, num_shards=256>,\n",
       "        'validation': <SplitInfo num_examples=5000, num_shards=8>,\n",
       "    },\n",
       "    citation=\"\"\"@article{DBLP:journals/corr/LinMBHPRDZ14,\n",
       "      author    = {Tsung{-}Yi Lin and\n",
       "                   Michael Maire and\n",
       "                   Serge J. Belongie and\n",
       "                   Lubomir D. Bourdev and\n",
       "                   Ross B. Girshick and\n",
       "                   James Hays and\n",
       "                   Pietro Perona and\n",
       "                   Deva Ramanan and\n",
       "                   Piotr Doll{'{a}}r and\n",
       "                   C. Lawrence Zitnick},\n",
       "      title     = {Microsoft {COCO:} Common Objects in Context},\n",
       "      journal   = {CoRR},\n",
       "      volume    = {abs/1405.0312},\n",
       "      year      = {2014},\n",
       "      url       = {http://arxiv.org/abs/1405.0312},\n",
       "      archivePrefix = {arXiv},\n",
       "      eprint    = {1405.0312},\n",
       "      timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},\n",
       "      biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},\n",
       "      bibsource = {dblp computer science bibliography, https://dblp.org}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coco_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py373",
   "language": "python",
   "name": "py373"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}