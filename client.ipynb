{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!/usr/bin/env python3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import socket\n",
    "import os\n",
    "import json\n",
    "import  time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from   nltk.translate.bleu_score import sentence_bleu\n",
    "import random\n",
    "import  re\n",
    "import sys\n",
    "import argparse\n",
    "import zlib\n",
    "import pickle5 as pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import  tensorflow_datasets as tfds\n",
    "import  functools\n",
    "\n",
    "from common.constants import test, BoxField, DatasetField\n",
    "from common.config import Config\n",
    "from common.logger import Logger\n",
    "from common.communication import Client\n",
    "from common.communication import Server\n",
    "from common.helper import ImagesInfo \n",
    "from common.timekeeper import TimeKeeper\n",
    "from common.helper import read_image, filt_text, get_predictions,process_predictions\n",
    "from CaptionModel import CaptionModel\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-s', '--server', action='store', type=str, required=False)\n",
    "parser.add_argument('-t', '--test_number', action='store', type=int, required=False)\n",
    "parser.add_argument('-l', '--split_layer', action='store', type=int, required=False)\n",
    "parser.add_argument('-v', '--verbose', action='store', type=int, required=False)\n",
    "parser.add_argument('-i', '--image_size', action='store', type=int, required=False)\n",
    "parser.add_argument('-m', '--max_tests', action='store', type=int, required=False)\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(args.server)\n",
    "\n",
    "server_ip = args.server\n",
    "test_number = args.test_number\n",
    "verbose = args.verbose\n",
    "image_size = args.image_size\n",
    "max_tests = args.max_tests\n",
    "split_layer = args.split_layer\n",
    "\n",
    "if(verbose == None):\n",
    "    verbose = 1\n",
    "\n",
    "if(split_layer == None):\n",
    "    split_layer = 3\n",
    "\n",
    "# test_number = 0\n",
    "if(test_number == None):\n",
    "    test_number = test.STANDALONE\n",
    "if(test_number == 0):\n",
    "    test_number = test.STANDALONE\n",
    "if(test_number == 1):\n",
    "    test_number = test.JPEG_TRANSFER\n",
    "if(test_number == 2):\n",
    "    test_number = test.DECODED_IMAGE_TRANSFER\n",
    "if(test_number == 3):\n",
    "    test_number = test.DECODED_IMAGE_TRANSFER_ZLIB\n",
    "if(test_number == 4):\n",
    "    test_number = test.SPLIT_LAYER_3\n",
    "if(test_number == 5):\n",
    "    test_number = test.SPLIT_LAYER_3_ZLIB\n",
    "\n",
    "test_scenarios = {  \n",
    "        test.STANDALONE:                    \"standalone processing at client device\", \n",
    "        test.JPEG_TRANSFER:                 \"Complete jpg file buffer transfer\", \n",
    "        test.DECODED_IMAGE_TRANSFER:        \"Decoded image buffer transfer\",\n",
    "        test.DECODED_IMAGE_TRANSFER_ZLIB:   \"Decoded image buffer transfer with zlib compression\",\n",
    "        test.SPLIT_LAYER_3:                 \"split model at layer 3\",\n",
    "        test.SPLIT_LAYER_3_ZLIB:            \"split model at layer 3 with zlib compression\",\n",
    "        }\n",
    "\n",
    "if(image_size == None):\n",
    "    image_size = 299\n",
    "\n",
    "if(max_tests == None):\n",
    "    max_tests = 50\n",
    "elif (((max_tests % 50) == 0) and (max_tests <= 5000)):\n",
    "    max_tests = max_tests\n",
    "else:\n",
    "    print(\"max_tests must be multiple of 50 and less than or equal to 5000\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Test scenario = %d %s\" % (test_number, test_scenarios[test_number]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "Test scenario = 0 standalone processing at client device\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "Logger.set_log_level(verbose)\n",
    "tk = TimeKeeper()\n",
    "cfg = Config(server_ip)\n",
    "client = Client(cfg)\n",
    "imagesInfo = ImagesInfo(cfg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from CaptionModel import CaptionModel\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "data_dir='/home/suphale/coco'\n",
    "N_LABELS = 80\n",
    "# split_val = \"validation\"\n",
    "split_val = \"validation[:20%]\"\n",
    "# split_val = \"validation[:1%]\"\n",
    "# h_image_height = 299\n",
    "# h_image_width = 299\n",
    "\n",
    "# split_layer = 3\n",
    "\n",
    "h_image_height = image_size\n",
    "h_image_width = image_size\n",
    "\n",
    "Logger.event_print(\"Test scenario   : %d %s\" % (test_number, test_scenarios[test_number]))\n",
    "Logger.event_print(\"Image shape     : (%d %d)\" % (h_image_height, h_image_width))\n",
    "Logger.event_print(\"Max tests       : %d\" % (max_tests))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[96mTest scenario   : 0 standalone processing at client device\u001b[0m\n",
      "\u001b[96mImage shape     : (250 250)\u001b[0m\n",
      "\u001b[96mMax tests       : 50\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def my_preprocess(inputs):\n",
    "    image = inputs['image']\n",
    "    image = tf.image.resize(image, (h_image_height, h_image_width))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 127.5\n",
    "    image -= 1.\n",
    "\n",
    "    targets = inputs['objects']\n",
    "    img_path = inputs['image/filename']\n",
    "\n",
    "    image_information = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    inputs = {DatasetField.IMAGES: image, DatasetField.IMAGES_INFO: image_information}\n",
    "\n",
    "    # ground_truths = {\n",
    "    #     BoxField.BOXES: targets[BoxField.BOXES] * tf.tile(image_information[tf.newaxis], [1, 2]),\n",
    "    #     BoxField.LABELS: tf.cast(targets[BoxField.LABELS], tf.int32),\n",
    "    #     BoxField.NUM_BOXES: tf.shape(targets[BoxField.LABELS]),\n",
    "    #     BoxField.WEIGHTS: tf.fill(tf.shape(targets[BoxField.LABELS]), 1.0)\n",
    "    # }\n",
    "    ground_truths = tf.cast(targets[BoxField.LABELS], tf.int32)\n",
    "    # ground_truths = tf.one_hot(ground_truths, depth=N_LABELS, dtype=tf.int32)\n",
    "    # ground_truths = tf.reduce_sum(ground_truths, 0)\n",
    "    # ground_truths = tf.greater( ground_truths, tf.constant( 0 ) )    \n",
    "    # ground_truths = tf.where (ground_truths, 1, 0) \n",
    "    return image, ground_truths, img_path\n",
    "\n",
    "def expand_dims_for_single_batch(image, ground_truths, img_path):\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    ground_truths = tf.expand_dims(ground_truths, axis=0)\n",
    "    return image, ground_truths, img_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# tf.compat.v1.disable_eager_execution()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "if(test_number in [test.STANDALONE]):\n",
    "    model = tf.keras.models.load_model(cfg.saved_model_path + '/model')\n",
    "    model = tf.keras.Model(inputs=model.inputs,outputs=[ \n",
    "                                model.layers[310].output, \n",
    "                                model.layers[313].output])    \n",
    "    captionModel = CaptionModel()\n",
    "if(test_number in [test.JPEG_TRANSFER, test.DECODED_IMAGE_TRANSFER, test.DECODED_IMAGE_TRANSFER_ZLIB]):\n",
    "    # head_model = tf.keras.models.load_model(cfg.saved_model_path + '/model')\n",
    "    send_json_dict = {}\n",
    "    send_json_dict['data_type'] = 'load_model_request'\n",
    "    send_json_dict['model'] = 'model'\n",
    "    app_json = json.dumps(send_json_dict)\n",
    "    response = client.send_load_model_request(str(app_json))\n",
    "    assert(response == 'OK')\n",
    "\n",
    "    send_json_dict = {}\n",
    "    send_json_dict['data_type'] = 'load_model_request'\n",
    "    send_json_dict['model'] = 'captionModel'\n",
    "    app_json = json.dumps(send_json_dict)\n",
    "    response = client.send_load_model_request(str(app_json))\n",
    "    assert(response == 'OK')\n",
    "\n",
    "\n",
    "if(test_number in [test.SPLIT_LAYER_3, test.SPLIT_LAYER_3_ZLIB]):\n",
    "    head_model = tf.keras.models.load_model(cfg.saved_model_path + '/head_model_'+ str(split_layer))\n",
    "    send_json_dict = {}\n",
    "    send_json_dict['data_type'] = 'load_model_request'\n",
    "    send_json_dict['model'] = 'tail_model'\n",
    "    send_json_dict['model_path'] = 'tail_model_' + str(split_layer)\n",
    "    app_json = json.dumps(send_json_dict)\n",
    "    response = client.send_load_model_request(str(app_json))\n",
    "    assert(response == 'OK')\n",
    "\n",
    "    send_json_dict = {}\n",
    "    send_json_dict['data_type'] = 'load_model_request'\n",
    "    send_json_dict['model'] = 'captionModel'\n",
    "    app_json = json.dumps(send_json_dict)\n",
    "    response = client.send_load_model_request(str(app_json))\n",
    "    assert(response == 'OK')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# @tf.function\n",
    "def handle_test_STANDALONE(sample_img_batch, img_path):\n",
    "    # print(ground_truth)\n",
    "    features, result = model(sample_img_batch)\n",
    "\n",
    "    features = tf.reshape(features, [sample_img_batch.shape[0],8*8, 2048])\n",
    "    caption_tensor = captionModel.evaluate(features)\n",
    "\n",
    "    tk.logInfo(img_path, tk.I_BUFFER_SIZE, 0)\n",
    "\n",
    "    tk.logTime(img_path, tk.E_START_COMMUNICATION)\n",
    "\n",
    "    tk.logTime(img_path, tk.E_STOP_COMMUNICATION)\n",
    "\n",
    "    predictions, predictions_prob = get_predictions(cfg, result)\n",
    "\n",
    "    tk.logInfo(img_path, tk.I_TAIL_MODEL_TIME, 0)\n",
    "\n",
    "    return predictions, predictions_prob, caption_tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def handle_test_JPEG_TRANSFER(file_name):\n",
    "    with open('/home/suphale/snehal_bucket/coco/raw-data/val2017/'+ file_name, 'rb') as file_t:\n",
    "        byte_buffer_to_send = bytearray(file_t.read())\n",
    "        send_json_dict = {}\n",
    "        send_json_dict['data_type'] = 'file'\n",
    "        send_json_dict['file_name'] = file_name\n",
    "        send_json_dict['data_size'] = (len(byte_buffer_to_send))\n",
    "        send_json_dict['data_shape'] = \"(%d,)\" % (len(byte_buffer_to_send))\n",
    "        # send_json_dict['data_buffer'] = blob_data\n",
    "\n",
    "        app_json = json.dumps(send_json_dict)\n",
    "\n",
    "        tk.logInfo(img_path, tk.I_BUFFER_SIZE, len(byte_buffer_to_send))\n",
    "\n",
    "        tk.logTime(img_path, tk.E_START_COMMUNICATION)\n",
    "\n",
    "        response = client.send_data(str(app_json), byte_buffer_to_send)\n",
    "\n",
    "        tk.logTime(img_path, tk.E_STOP_COMMUNICATION)\n",
    "\n",
    "        response = json.loads(response)\n",
    "\n",
    "        predictions = response['predictions']\n",
    "        predictions_prob = response['predictions_prob']\n",
    "        caption_tensor = response['predicted_captions']\n",
    "        # predictions = pickle.loads(predictions)\n",
    "        tail_model_time = response['tail_model_time']\n",
    "        tk.logInfo(img_path, tk.I_TAIL_MODEL_TIME, tail_model_time)\n",
    "\n",
    "        return predictions, predictions_prob, caption_tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def handle_test_DECODED_IMAGE_TRANSFER(image_tensor,file_name, zlib_compression=False):\n",
    "    # image_tensor = read_image(file_name)\n",
    "    # image_tensor = tf.expand_dims(image_tensor, 0) \n",
    "\n",
    "    image_np_array = image_tensor.numpy()\n",
    "\n",
    "    byte_buffer_to_send = image_np_array.tobytes()\n",
    "    if(zlib_compression == True):\n",
    "        byte_buffer_to_send = zlib.compress(byte_buffer_to_send)\n",
    "\n",
    "    type(byte_buffer_to_send)\n",
    "\n",
    "    send_json_dict = {}\n",
    "    send_json_dict['data_type'] = 'data'\n",
    "    send_json_dict['file_name'] = file_name\n",
    "    send_json_dict['data_size'] = (len(byte_buffer_to_send))\n",
    "    send_json_dict['data_shape'] = image_np_array.shape\n",
    "    if(zlib_compression == True):\n",
    "        send_json_dict['zlib_compression'] = 'yes'\n",
    "    else:\n",
    "        send_json_dict['zlib_compression'] = 'no'\n",
    "\n",
    "    app_json = json.dumps(send_json_dict)\n",
    "\n",
    "    tk.logInfo(img_path, tk.I_BUFFER_SIZE, len(byte_buffer_to_send))\n",
    "\n",
    "    tk.logTime(img_path, tk.E_START_COMMUNICATION)\n",
    "\n",
    "    response = client.send_data(str(app_json), byte_buffer_to_send)\n",
    "\n",
    "    tk.logTime(img_path, tk.E_STOP_COMMUNICATION)\n",
    "\n",
    "    response = json.loads(response)\n",
    "\n",
    "    predictions = response['predictions']\n",
    "    predictions_prob = response['predictions_prob']\n",
    "    tail_model_time = response['tail_model_time']\n",
    "    caption_tensor = response['predicted_captions']\n",
    "    tk.logInfo(img_path, tk.I_TAIL_MODEL_TIME, tail_model_time)\n",
    "\n",
    "    return predictions, predictions_prob, caption_tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def handle_test_SPLIT_LAYER_3(image_tensor,file_name, zlib_compression=False):\n",
    "\n",
    "    # temp_input = tf.expand_dims(read_image(file_name), 0) \n",
    "    intermediate_tensor = head_model(image_tensor)\n",
    "    image_np_array = intermediate_tensor.numpy()\n",
    "\n",
    "    byte_buffer_to_send = image_np_array.tobytes()\n",
    "    if(zlib_compression == True):\n",
    "        byte_buffer_to_send = zlib.compress(byte_buffer_to_send)\n",
    "\n",
    "    type(byte_buffer_to_send)\n",
    "\n",
    "    send_json_dict = {}\n",
    "    send_json_dict['data_type'] = 'data'\n",
    "    send_json_dict['file_name'] = file_name\n",
    "    send_json_dict['data_size'] = (len(byte_buffer_to_send))\n",
    "    send_json_dict['data_shape'] = image_np_array.shape\n",
    "    if(zlib_compression == True):\n",
    "        send_json_dict['zlib_compression'] = 'yes'\n",
    "    else:\n",
    "        send_json_dict['zlib_compression'] = 'no'\n",
    "\n",
    "    app_json = json.dumps(send_json_dict)\n",
    "\n",
    "    tk.logInfo(img_path, tk.I_BUFFER_SIZE, len(byte_buffer_to_send))\n",
    "\n",
    "    tk.logTime(img_path, tk.E_START_COMMUNICATION)\n",
    "\n",
    "    response = client.send_data(str(app_json), byte_buffer_to_send)\n",
    "\n",
    "    tk.logTime(img_path, tk.E_STOP_COMMUNICATION)\n",
    "\n",
    "    response = json.loads(response)\n",
    "\n",
    "    predictions = response['predictions']\n",
    "    predictions_prob = response['predictions_prob']\n",
    "    caption_tensor = response['predicted_captions']\n",
    "    tail_model_time = response['tail_model_time']\n",
    "    tk.logInfo(img_path, tk.I_TAIL_MODEL_TIME, tail_model_time)\n",
    "\n",
    "    return predictions, predictions_prob, caption_tensor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def process_caption_predictions(caption_tensor, img_path):\n",
    "    pred_caption=' '.join(caption_tensor).rsplit(' ', 1)[0]\n",
    "    real_appn = []\n",
    "    real_caption_list = imagesInfo.annotations_dict[img_path]\n",
    "    for real_caption in real_caption_list:\n",
    "        real_caption=filt_text(real_caption)\n",
    "        real_appn.append(real_caption.split())\n",
    "    reference = real_appn\n",
    "    candidate = pred_caption.split()\n",
    "    score = sentence_bleu(reference, candidate, weights=[1]) #set your weights)\n",
    "    return score,real_caption,pred_caption"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "ds_val, ds_info = tfds.load(name=\"coco/2017\", split=split_val, data_dir=data_dir, shuffle_files=False, download=False, with_info=True)\n",
    "ds_val = ds_val.map(functools.partial(my_preprocess), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f66d4b9f680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function my_preprocess at 0x7f66d4b9f680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: AutoGraph could not transform <function my_preprocess at 0x7f66d4b9f680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "count = 0\n",
    "max_test_images = max_tests\n",
    "\n",
    "# coco_image_dir = '/home/suphale/snehal_bucket/coco/raw-data/val2017/'\n",
    "\n",
    "total_time = 0.0\n",
    "\n",
    "total_time = 0.0\n",
    "df = pd.DataFrame(columns=['img_path','ground_truth', 'top_predict', 'Prediction', 'accuracy', 'top_1_accuracy', 'top_5_accuracy', 'precision', 'recall', 'time'])\n",
    "ds_val = ds_val.take(max_tests)\n",
    "for sample_img_batch, ground_truth, img_path in tqdm(ds_val):\n",
    "# for sample_img_batch, ground_truth, img_path in ds_val:\n",
    "    count += 1\n",
    "    img_path = img_path.numpy().decode()\n",
    "\n",
    "    tk.startRecord(img_path)\n",
    "    tk.logTime(img_path, tk.E_START_CLIENT_PROCESSING)\n",
    "\n",
    "    tensor_shape = len(ground_truth.get_shape().as_list())\n",
    "    if(tensor_shape > 1):\n",
    "        ground_truth = tf.squeeze(ground_truth,[0])\n",
    "\n",
    "    ground_truth = list(set(ground_truth.numpy()))\n",
    "    # print(ground_truth)\n",
    "\n",
    "    # accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(sample_img_batch, ground_truth)\n",
    "\n",
    "    if(test_number == test.STANDALONE):\n",
    "        predictions,predictions_prob, caption_tensor = handle_test_STANDALONE(sample_img_batch, img_path)\n",
    "    if(test_number == test.JPEG_TRANSFER):\n",
    "        predictions,predictions_prob, caption_tensor = handle_test_JPEG_TRANSFER(img_path)\n",
    "    if(test_number == test.DECODED_IMAGE_TRANSFER):\n",
    "        predictions,predictions_prob, caption_tensor = handle_test_DECODED_IMAGE_TRANSFER(sample_img_batch, img_path)\n",
    "    if(test_number == test.DECODED_IMAGE_TRANSFER_ZLIB):\n",
    "        predictions,predictions_prob, caption_tensor = handle_test_DECODED_IMAGE_TRANSFER(sample_img_batch, img_path,zlib_compression=True)\n",
    "    if(test_number == test.SPLIT_LAYER_3):\n",
    "        predictions,predictions_prob, caption_tensor = handle_test_SPLIT_LAYER_3(sample_img_batch, img_path)\n",
    "    if(test_number == test.SPLIT_LAYER_3_ZLIB):\n",
    "        predictions,predictions_prob, caption_tensor = handle_test_SPLIT_LAYER_3(sample_img_batch, img_path,zlib_compression=True)\n",
    "\n",
    "    tk.logTime(img_path, tk.E_STOP_CLIENT_PROCESSING)\n",
    "\n",
    "    accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(cfg, imagesInfo, ground_truth,predictions, predictions_prob)\n",
    "    bleu_score,real_caption,pred_caption = process_caption_predictions(caption_tensor, img_path)\n",
    "\n",
    "    df = df.append(\n",
    "        {'image':img_path, \n",
    "        'ground_truth':(str(imagesInfo.get_segmentation_texts(ground_truth))),\n",
    "        'top_predict':str(top_predictions),\n",
    "        'Prediction':predictions_str,\n",
    "        'accuracy':accuracy,\n",
    "        'top_1_accuracy':top_1_accuracy,\n",
    "        'top_5_accuracy':top_5_accuracy,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'BLEU':bleu_score,\n",
    "        'real_caption':real_caption,\n",
    "        'pred_caption':pred_caption,\n",
    "        'time':0,\n",
    "        },\n",
    "        ignore_index = True)\n",
    "    truth_str = ' '.join([str(elem) for elem in imagesInfo.get_segmentation_texts(ground_truth)])\n",
    "    # Logger.debug_print(\"ground_truth  : %s\" % (truth_str))\n",
    "    # Logger.debug_print(\"Prediction    : %s\" % (predictions_str))\n",
    "\n",
    "    tk.finishRecord(img_path)\n",
    "\n",
    "df.to_csv(cfg.temp_path + '/results_'+cfg.timestr+'.csv')\n",
    "av_column = df.mean(axis=0)\n",
    "\n",
    "Logger.milestone_print(\"----------------:\")\n",
    "Logger.milestone_print(\"Test scenario   : %d %s\" % (test_number, test_scenarios[test_number]))\n",
    "Logger.milestone_print(\"Image shape     : (%d %d)\" % (h_image_height, h_image_width))\n",
    "Logger.milestone_print(\"Max tests       : %d\" % (max_tests))\n",
    "Logger.milestone_print(\"accuracy        : %.2f\" % (av_column.accuracy))\n",
    "Logger.milestone_print(\"top_1_accuracy  : %.2f\" % (av_column.top_1_accuracy))\n",
    "Logger.milestone_print(\"top_5_accuracy  : %.2f\" % (av_column.top_5_accuracy))\n",
    "Logger.milestone_print(\"precision       : %.2f\" % (av_column.precision))\n",
    "Logger.milestone_print(\"recall          : %.2f\" % (av_column.recall))\n",
    "Logger.milestone_print(\"BLEU            : %.2f\" % (av_column.BLEU))\n",
    "Logger.milestone_print(\"time            : %.2f\" % (av_column.time))\n",
    "\n",
    "# tk.printAll()\n",
    "tk.summary()\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "'size' must be a 1-D Tensor of 2 elements: new_height, new_width",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8b83cb960ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTANDALONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_test_STANDALONE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_img_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJPEG_TRANSFER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_test_JPEG_TRANSFER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-be2a8ef6d1ef>\u001b[0m in \u001b[0;36mhandle_test_STANDALONE\u001b[0;34m(sample_img_batch, img_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample_img_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_img_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# features = tf.reshape(features, [sample_img_batch.shape[0], 4*4, 2048])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py373/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py373/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images_v2\u001b[0;34m(images, size, method, preserve_aspect_ratio, antialias, name)\u001b[0m\n\u001b[1;32m   1645\u001b[0m       \u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m       skip_resize_if_same=False)\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py373/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_images_common\u001b[0;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[1;32m   1327\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\'size\\' must be a 1-D int32 Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m       raise ValueError('\\'size\\' must be a 1-D Tensor of 2 elements: '\n\u001b[0m\u001b[1;32m   1330\u001b[0m                        'new_height, new_width')\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'size' must be a 1-D Tensor of 2 elements: new_height, new_width"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ds_info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Test = False\n",
    "if (Test == True):\n",
    "    ds_val = ds_val.take(1)\n",
    "    for sample_img_batch, ground_truth, img_path in tqdm(ds_val):\n",
    "        count += 1\n",
    "\n",
    "        tensor_shape = len(ground_truth.get_shape().as_list())\n",
    "        if(tensor_shape > 1):\n",
    "            ground_truth = tf.squeeze(ground_truth,[0])\n",
    "        ground_truth = list(set(ground_truth.numpy()))\n",
    "\n",
    "        img_path = img_path.numpy().decode()\n",
    "        print(img_path)\n",
    "        features, result = model(sample_img_batch)\n",
    "        predictions, predictions_prob = get_predictions(cfg, result)\n",
    "        accuracy, top_1_accuracy,top_5_accuracy,precision,recall, top_predictions, predictions_str = process_predictions(cfg, imagesInfo, ground_truth,predictions, predictions_prob)\n",
    "        print(predictions_str)\n",
    "\n",
    "        features = tf.reshape(features, [sample_img_batch.shape[0],8*8, 2048])\n",
    "        caption_tensor = captionModel.evaluate(features)\n",
    "\n",
    "        print(type(caption_tensor))\n",
    "        score,real_caption,pred_caption = process_caption_predictions(caption_tensor, img_path)\n",
    "\n",
    "        print(\"BLEU: %.2f\" % (score))\n",
    "        print ('Real:', real_caption)\n",
    "        print ('Pred:', pred_caption)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imagesInfo.annotations_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model.save(cfg.temp_path + '/extractor_model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d8a323e6010706682c07af791323eacfc072764aa514c33420848fded080be"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}