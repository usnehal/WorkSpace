{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_UhVVpfP_PQ"
   },
   "source": [
    "# Creating TFRecords\n",
    "\n",
    "**Author:** [Dimitre Oliveira](https://www.linkedin.com/in/dimitre-oliveira-7a1a0113a/)<br>\n",
    "**Date created:** 2021/02/27<br>\n",
    "**Last modified:** 2021/02/27<br>\n",
    "**Description:** Converting data to the TFRecord format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6mwUwA2P_Pe"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The TFRecord format is a simple format for storing a sequence of binary records.\n",
    "Converting your data into TFRecord has many advantages, such as:\n",
    "\n",
    "- **More efficient storage**: the TFRecord data can take up less space than the original\n",
    "data; it can also be partitioned into multiple files.\n",
    "- **Fast I/O**: the TFRecord format can be read with parallel I/O operations, which is\n",
    "useful for [TPUs](https://www.tensorflow.org/guide/tpu) or multiple hosts.\n",
    "- **Self-contained files**: the TFRecord data can be read from a single sourceâ€”for\n",
    "example, the [COCO2017](https://cocodataset.org/) dataset originally stores data in\n",
    "two folders (\"images\" and \"annotations\").\n",
    "\n",
    "An important use case of the TFRecord data format  is training on TPUs. First, TPUs are\n",
    "fast enough to benefit from optimized I/O operations. In addition, TPUs require\n",
    "data to be stored remotely (e.g. on Google Cloud Storage) and using the TFRecord format\n",
    "makes it easier to load the data without batch-downloading.\n",
    "\n",
    "Performance using the TFRecord format can be further improved if you also use\n",
    "it with the [tf.data](https://www.tensorflow.org/guide/data) API.\n",
    "\n",
    "In this example you will learn how to convert data of different types (image, text, and\n",
    "numeric) into TFRecord.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
    "\n",
    "\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XoRWUr-2P_Pr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q0J_iIjP_Pz"
   },
   "source": [
    "## Download the COCO2017 dataset\n",
    "\n",
    "We will be using the [COCO2017](https://cocodataset.org/) dataset, because it has many\n",
    "different types of features, including images, floating point data, and lists.\n",
    "It will serve as a good example of how to encode different features into the TFRecord\n",
    "format.\n",
    "\n",
    "This dataset has two sets of fields: images and annotation meta-data.\n",
    "\n",
    "The images are a collection of JPG files and the meta-data are stored in a JSON file\n",
    "which, according to the [official site](https://cocodataset.org/#format-data),\n",
    "contains the following properties:\n",
    "\n",
    "```\n",
    "id: int,\n",
    "image_id: int,\n",
    "category_id: int,\n",
    "segmentation: RLE or [polygon], object segmentation mask\n",
    "bbox: [x,y,width,height], object bounding box coordinates\n",
    "area: float, area of the bounding box\n",
    "iscrowd: 0 or 1, is single object or a collection\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-q0_oopAP_P1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The COCO dataset has been downloaded and extracted successfully.\n",
      "Number of train images: 860001\n",
      "Number of val images: 36781\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"datasets\"\n",
    "tfrecords_dir = \"tfrecords\"\n",
    "train_images_dir = os.path.join(root_dir, \"train2017\")\n",
    "val_images_dir = os.path.join(root_dir, \"val2017\")\n",
    "\n",
    "annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "train_annotation_file = os.path.join(annotations_dir, \"instances_train2017.json\")\n",
    "val_annotation_file = os.path.join(annotations_dir, \"instances_val2017.json\")\n",
    "\n",
    "train_captions_annotation_file = os.path.join(annotations_dir, \"captions_train2017.json\")\n",
    "val_captions_annotation_file = os.path.join(annotations_dir, \"captions_val2017.json\")\n",
    "\n",
    "train_images_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
    "val_images_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "annotations_url = (\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "\n",
    "# Download image files\n",
    "if not os.path.exists(val_images_dir):\n",
    "    val_image_zip = tf.keras.utils.get_file(\n",
    "        \"images.zip\", cache_dir=os.path.abspath(\".\"), origin=val_images_url, extract=True,\n",
    "    )\n",
    "    os.remove(val_image_zip)\n",
    "\n",
    "# Download image files\n",
    "if not os.path.exists(train_images_dir):\n",
    "    train_image_zip = tf.keras.utils.get_file(\n",
    "        \"train_images.zip\", cache_dir=os.path.abspath(\".\"), origin=train_images_url, extract=True,\n",
    "    )\n",
    "    os.remove(train_image_zip)\n",
    "\n",
    "# Download caption annotation files\n",
    "if not os.path.exists(annotations_dir):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        \"captions.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=annotations_url,\n",
    "        extract=True,\n",
    "    )\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "    \n",
    "print(\"The COCO dataset has been downloaded and extracted successfully.\")\n",
    "\n",
    "with open(train_annotation_file, \"r\") as f:\n",
    "    train_annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "with open(val_annotation_file, \"r\") as f:\n",
    "    val_annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "with open(train_captions_annotation_file, \"r\") as f:\n",
    "    train_captions_annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "with open(val_captions_annotation_file, \"r\") as f:\n",
    "    val_captions_annotations = json.load(f)[\"annotations\"]\n",
    "    \n",
    "print(f\"Number of train images: {len(train_annotations)}\")\n",
    "print(f\"Number of val images: {len(val_annotations)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V69qKdapP_P4"
   },
   "source": [
    "### Contents of the COCO2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1FXtfqpqP_P5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> A bicycle replica with a clock as the front wheel. <end>\n",
      "<start> The bike has a clock as a tire. <end>\n",
      "<start> A black metal bicycle with a clock inside the front wheel. <end>\n",
      "<start> A bicycle figurine in which the front wheel is replaced with a clock <end>\n",
      "<start> A clock with the appearance of the wheel of a bicycle <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pprint.pprint(train_captions_annotations[0])\n",
    "def getCaptions(captions_annotations,image_id):\n",
    "    str = \"\"\n",
    "    for a in train_captions_annotations:\n",
    "        if(a['image_id'] == 203564):\n",
    "            str += '<start> ' + a['caption'].rstrip() + ' <end>' + '\\n'\n",
    "    return str\n",
    "str = getCaptions(train_captions_annotations, 203564)\n",
    "print(str)\n",
    "# train_captions_annotations[]\n",
    "# train_captions_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXFgfftqP_P6"
   },
   "source": [
    "## Parameters\n",
    "\n",
    "`num_samples` is the number of data samples on each TFRecord file.\n",
    "\n",
    "`num_tfrecods` is total number of TFRecords that we will create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RgsUehMYP_P7"
   },
   "outputs": [],
   "source": [
    "num_samples = 4096\n",
    "num_train_tfrecods = len(train_annotations) // num_samples\n",
    "num_val_tfrecods = len(val_annotations) // num_samples\n",
    "if len(train_annotations) % num_samples:\n",
    "    num_train_tfrecods += 1  # add one record if there are any remaining samples\n",
    "if len(val_annotations) % num_samples:\n",
    "    num_val_tfrecods += 1  # add one record if there are any remaining samples\n",
    "\n",
    "if not os.path.exists(tfrecords_dir):\n",
    "    os.makedirs(tfrecords_dir)  # creating TFRecords output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-31 13:13:34.896629: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-31 13:13:34.896884: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-31 13:13:34.898336: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "split_layer = 100\n",
    "saved_model_path = '/home/suphale/WorkSpace/saved_model'\n",
    "model_path = saved_model_path + '/iv3_head_model_%d' % (split_layer)\n",
    "head_model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "model_path = saved_model_path + '/iv3_tail_model_%d' % (split_layer)\n",
    "tail_model = tf.keras.models.load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5k_urfrP_P8"
   },
   "source": [
    "## Define TFRecords helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vypG9KbGP_P9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n",
    "    )\n",
    "\n",
    "def tensor_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[value.numpy().tobytes()])\n",
    "    )\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def create_example(image, path, example,captions, h):\n",
    "    feature = {\n",
    "        \"image\": image_feature(image),\n",
    "        \"tensor\": tensor_feature(h),\n",
    "        \"path\": bytes_feature(path),\n",
    "        \"area\": float_feature(example[\"area\"]),\n",
    "        \"bbox\": float_feature_list(example[\"bbox\"]),\n",
    "        \"category_id\": int64_feature(example[\"category_id\"]),\n",
    "        \"id\": int64_feature(example[\"id\"]),\n",
    "        \"image_id\": int64_feature(example[\"image_id\"]),\n",
    "        \"captions\": bytes_feature(captions),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"tensor\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"area\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"bbox\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"category_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"image_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"captions\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    example[\"bbox\"] = tf.sparse.to_dense(example[\"bbox\"])\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN4lMVoZP_QA"
   },
   "source": [
    "## Generate data in the TFRecord format\n",
    "\n",
    "Let's generate the COCO2017 data in the TFRecord format. The format will be\n",
    "`file_{number}.tfrec` (this is optional, but including the number sequences in the file\n",
    "names can make counting easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tfrecods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "J1YgKJnyP_QB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_tfrecods=50\n",
      "num_val_tfrecods=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:40<00:00,  8.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:40<00:00,  8.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:42<00:00,  8.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:41<00:00,  8.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:41<00:00,  8.87it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:40<00:00,  8.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:38<00:00,  8.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4096/4096 [07:36<00:00,  8.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4013/4013 [07:30<00:00,  8.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# num_tfrecods = 1\n",
    "num_train_tfrecods = 50\n",
    "# num_val_tfrecods = 1\n",
    "print(\"num_train_tfrecods=%d\" % num_train_tfrecods)\n",
    "print(\"num_val_tfrecods=%d\" % num_val_tfrecods)\n",
    "\n",
    "if(False):\n",
    "    for tfrec_num in range(num_train_tfrecods):\n",
    "        samples = train_annotations[(tfrec_num * num_samples) : ((tfrec_num + 1) * num_samples)]\n",
    "        options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "        with tf.io.TFRecordWriter(\n",
    "            tfrecords_dir + \"/file_train_%.2i-%i.tfrec\" % (tfrec_num, len(samples)),options=options\n",
    "        ) as writer:\n",
    "            for sample in tqdm(samples):\n",
    "                image_path = f\"{train_images_dir}/{sample['image_id']:012d}.jpg\"\n",
    "                image = tf.io.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "                captions = getCaptions(train_captions_annotations, sample['image_id'])\n",
    "                batch_image = tf.image.resize(image, (250, 250))\n",
    "                batch_image = tf.expand_dims(batch_image, 0) \n",
    "                h = head_model(batch_image)\n",
    "                example = create_example(image, image_path, sample,captions, h)\n",
    "                writer.write(example.SerializeToString())\n",
    "                \n",
    "if(True):\n",
    "    for tfrec_num in range(num_val_tfrecods):\n",
    "        samples = val_annotations[(tfrec_num * num_samples) : ((tfrec_num + 1) * num_samples)]\n",
    "        options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "\n",
    "        with tf.io.TFRecordWriter(\n",
    "            tfrecords_dir + \"/file_val_%.2i-%i.tfrec\" % (tfrec_num, len(samples)),options=options\n",
    "        ) as writer:\n",
    "            for sample in tqdm(samples):\n",
    "                image_path = f\"{val_images_dir}/{sample['image_id']:012d}.jpg\"\n",
    "                image = tf.io.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "                captions = getCaptions(val_captions_annotations, sample['image_id'])\n",
    "                batch_image = tf.image.resize(image, (250, 250))\n",
    "                batch_image = tf.expand_dims(batch_image, 0) \n",
    "                h = head_model(batch_image)\n",
    "                example = create_example(image, image_path, sample,captions,h)\n",
    "                writer.write(example.SerializeToString())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([250, 250,   3], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.io.decode_jpeg(tf.io.read_file('./datasets/val2017/000000403353.jpg'),channels=3)\n",
    "image = tf.image.resize(image, (250, 250))\n",
    "tf.shape(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1,  13,  13, 768], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_image = tf.expand_dims(image, 0) \n",
    "h = head_model(batch_image)\n",
    "tf.shape(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = tf.io.encode_jpeg(image).numpy()\n",
    "# type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  1  13  13 768], shape=(4,), dtype=int32)\n",
      "<class 'bytes'>\n",
      "tf.Tensor([  1  13  13 768], shape=(4,), dtype=int32)\n",
      "tf.Tensor([  1  13  13 768], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from   numpy import float32\n",
    "from   numpy import byte\n",
    "\n",
    "print(tf.shape(h))\n",
    "bytes_list=h.numpy().tobytes()\n",
    "print(type(bytes_list))\n",
    "# h_new = tf.reshape(bytes_list,[  1  ,13,  13, 768])\n",
    "# b = tf.io.decode_raw(bytes_list, tf.int32)\n",
    "# h_tensor = tf.Tensor(bytes_list)\n",
    "# h_new = tf.reshape(b,[  1  ,28,  38, 768])\n",
    "# print(tf.shape(h_new))\n",
    "\n",
    "generated_np_array = np.frombuffer(bytes_list, dtype=np.int32)\n",
    "# generated_np_array = np.frombuffer(generated_np_array, dtype=np.int32)\n",
    "generated_image_np_array = generated_np_array.reshape([  1  ,13,  13, 768])\n",
    "image_tensor = tf.convert_to_tensor(generated_image_np_array, dtype=tf.int32)\n",
    "print(tf.shape(h))\n",
    "print(tf.shape(image_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XmMsUtXP_QF"
   },
   "source": [
    "## Explore one sample from the generated TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_tfrecord_fn at 0x7f765257ce18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_tfrecord_fn at 0x7f765257ce18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "bbox: [199.84 200.46  77.71  70.88]\n",
      "area: 2765.148681640625\n",
      "captions: b'<start> A bicycle replica with a clock as the front wheel. <end>\\n<start> The bike has a clock as a tire. <end>\\n<start> A black metal bicycle with a clock inside the front wheel. <end>\\n<start> A bicycle figurine in which the front wheel is replaced with a clock <end>\\n<start> A clock with the appearance of the wheel of a bicycle <end>\\n'\n",
      "category_id: 58\n",
      "id: 156\n",
      "image_id: 558840\n",
      "path: b'datasets/train2017/000000558840.jpg'\n",
      "Image shape: (427, 640, 3)\n",
      "Tensor shape: ()\n",
      "tf.Tensor([], shape=(0,), dtype=int32)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "519168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-31 20:56:54.873322: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-31 20:56:54.873813: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2800210000 Hz\n"
     ]
    }
   ],
   "source": [
    "print(num_samples)\n",
    "train_raw_dataset = tf.data.TFRecordDataset(f\"{tfrecords_dir}/file_train_00-{num_samples}.tfrec\",compression_type='GZIP')\n",
    "train_parsed_dataset = train_raw_dataset.map(parse_tfrecord_fn)\n",
    "test_bytes = None\n",
    "for features in train_parsed_dataset.take(1):\n",
    "    for key in features.keys():\n",
    "        if key != \"image\" and key != \"tensor\":\n",
    "            print(f\"{key}: {features[key]}\")\n",
    "\n",
    "    print(f\"Image shape: {features['image'].shape}\")\n",
    "    print(f\"Tensor shape: {features['tensor'].shape}\")\n",
    "    print(tf.shape(features[\"tensor\"].numpy()))\n",
    "    test_bytes = features[\"tensor\"]\n",
    "    print(type(test_bytes))\n",
    "    bytes_list = features[\"tensor\"].numpy()\n",
    "    print(len(bytes_list))\n",
    "    generated_np_array = np.frombuffer(bytes_list, dtype=np.int32)\n",
    "    generated_image_np_array = generated_np_array.reshape([  1  ,13,  13, 768])\n",
    "    image_tensor = tf.convert_to_tensor(generated_image_np_array, dtype=tf.int32)\n",
    "\n",
    "#     h_new = tf.reshape(bytes_list,[  1  ,28,  38, 768])\n",
    "    \n",
    "#     h_tensor = tf.Tensor(bytes_list)\n",
    "#     h_new = tf.reshape(bytes_list,[  1  ,28,  38, 768])\n",
    "\n",
    "#     plt.figure(figsize=(7, 7))\n",
    "#     plt.imshow(features[\"image\"].numpy())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GWvm46InP_QI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbox: [199.84 200.46  77.71  70.88]\n",
      "area: 2765.148681640625\n",
      "captions: b'<start> A bicycle replica with a clock as the front wheel. <end>\\n<start> The bike has a clock as a tire. <end>\\n<start> A black metal bicycle with a clock inside the front wheel. <end>\\n<start> A bicycle figurine in which the front wheel is replaced with a clock <end>\\n<start> A clock with the appearance of the wheel of a bicycle <end>\\n'\n",
      "category_id: 58\n",
      "id: 156\n",
      "image_id: 558840\n",
      "path: b'datasets/train2017/000000558840.jpg'\n",
      "Tensor shape: ()\n"
     ]
    }
   ],
   "source": [
    "val_raw_dataset = tf.data.TFRecordDataset(f\"{tfrecords_dir}/file_train_00-{num_samples}.tfrec\",compression_type='GZIP')\n",
    "val_parsed_dataset = val_raw_dataset.map(parse_tfrecord_fn)\n",
    "\n",
    "for features in val_parsed_dataset.take(1):\n",
    "    for key in features.keys():\n",
    "        if key != \"image\" and key != \"tensor\":\n",
    "            print(f\"{key}: {features[key]}\")\n",
    "\n",
    "#     print(f\"Image shape: {features['image'].shape}\")\n",
    "    print(f\"Tensor shape: {features['tensor'].shape}\")\n",
    "#     print(len(features[\"tensor\"].numpy()))\n",
    "#     print(type(features[\"tensor\"]))\n",
    "#     h = \n",
    "    \n",
    "#     new_tensor = tf.reshape(features[\"tensor\"].numpy(),[  1  ,28,  38, 768])\n",
    "#     plt.figure(figsize=(7, 7))\n",
    "#     plt.imshow(features[\"image\"].numpy())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAPwDOGLP_QK"
   },
   "source": [
    "## Train a simple model using the generated TFRecords\n",
    "\n",
    "Another advantage of TFRecord is that you are able to add many features to it and later\n",
    "use only a few of them, in this case, we are going to use only `image` and `category_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kneXjEHJP_QK"
   },
   "source": [
    "## Define dataset helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "latent_dim = 10 * 1024 \n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(13 * 13 * 768 , activation='sigmoid'),\n",
    "      layers.Reshape((13, 13, 768))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 13, 13, 768)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 256)       1769728   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 5, 256)         262400    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 13, 13, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 768)       1770240   \n",
      "=================================================================\n",
      "Total params: 4,982,528\n",
      "Trainable params: 4,982,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(shape=(13, 13, 768))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(256, (3, 3), activation=\"relu\")(input)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = layers.Conv2D(256, (2, 2), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Conv2DTranspose(256, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(256, (3, 3), strides=2, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(768, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, x)\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "v9Du851lP_QL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27787/2194655688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#     steps_per_epoch=steps_per_epoch,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/py373/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py373/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py373/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    862\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         raise ValueError(\"Creating variables on a non-first call to a function\"\n\u001b[0m\u001b[1;32m    865\u001b[0m                          \" decorated with tf.function.\")\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def prepare_sample(features):\n",
    "    bytes_list = features[\"tensor\"]\n",
    "    dr = tf.io.decode_raw(bytes_list, tf.int32)\n",
    "    image_tensor = tf.reshape(dr, [  13,  13, 768])\n",
    "    return image_tensor, image_tensor\n",
    "\n",
    "\n",
    "def get_dataset(filenames, batch_size):\n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE,compression_type='GZIP')\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTOTUNE)\n",
    "        .map(prepare_sample, num_parallel_calls=AUTOTUNE)\n",
    "        .shuffle(batch_size * 10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_train_0*.tfrec\")\n",
    "val_filenames = tf.io.gfile.glob(f\"{tfrecords_dir}/file_val_0*.tfrec\")\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "steps_per_epoch = 50\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "input_tensor = tf.keras.layers.Input(shape=(224, 224, 3), name=\"image\")\n",
    "model = tf.keras.applications.EfficientNetB0(\n",
    "    input_tensor=input_tensor, weights=None, classes=91\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "autoencoder.fit(\n",
    "    x=get_dataset(train_filenames, batch_size),\n",
    "    validation_data=(get_dataset(val_filenames, batch_size)),\n",
    "    epochs=epochs,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALx9808LP_QN"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This example demonstrates that instead of reading images and annotations from different\n",
    "sources you can have your data coming from a single source thanks to TFRecord.\n",
    "This process can make storing and reading data simpler and more efficient.\n",
    "For more information, you can go to the [TFRecord and\n",
    "tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights(\"./temp/autoencoder.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-01 00:10:02.695052: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./temp/auto/assets\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save(\"./temp/auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto = tf.keras.\n",
    "auto = tf.keras.models.load_model(\"./temp/auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "creating_tfrecords",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "53d8a323e6010706682c07af791323eacfc072764aa514c33420848fded080be"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
