{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from   tensorflow.keras import layers,Model\n",
    "import pickle5 as pickle\n",
    "from   tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from   tensorflow.keras.activations import tanh\n",
    "from   tensorflow.keras.activations import softmax\n",
    "from   numpy import float32\n",
    "import json\n",
    "import time\n",
    "\n",
    "from Config import Config\n",
    "import Logger\n",
    "from Communication import Server\n",
    "import Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # build your Dense layer with relu activation\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation='relu')\n",
    "        \n",
    "    def call(self, features):\n",
    "        # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "        features = self.dense(features)\n",
    "        return features    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.units=units\n",
    "        # build your Dense layer\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        # build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # build your final Dense layer with unit 1\n",
    "        # self.V = tf.keras.layers.Dense(1, activation='softmax')\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features shape: (batch_size, 8*8, embedding_dim)\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        score = tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        score = self.V(score)\n",
    "        attention_weights = softmax(score, axis=1)\n",
    "\n",
    "        # shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        # reduce the shape to (batch_size, embedding_dim)\n",
    "        # context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.reduce_mean(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size, cfg):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.units = units\n",
    "        # iniitalise your Attention model with units\n",
    "        self.attention = Attention_model(self.units)\n",
    "        # build your Embedding layer\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, self.cfg.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        # build your Dense layer\n",
    "        self.d1 = tf.keras.layers.Dense(self.units)\n",
    "        # build your Dense layer\n",
    "        self.d2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        #create your context vector & attention weights from attention model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        # embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed = self.embed(x)\n",
    "        # Concatenate your input with the context vector from attention layer. \n",
    "        # Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)\n",
    "        # Extract the output & hidden state from GRU layer. \n",
    "        # Output shape : (batch_size, max_length, hidden_size)\n",
    "        output, state = self.gru(embed)\n",
    "        output = self.d1(output)\n",
    "        # shape : (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) \n",
    "        # shape : (batch_size * max_length, vocab_size)\n",
    "        output = self.d2(output)\n",
    "\n",
    "        return output, state, attention_weights\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TailModel:\n",
    "    def __init__(self,cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "        with open(self.cfg.saved_model_path + '/tokenizer.pickle', 'rb') as handle:\n",
    "            self.tokenizer = pickle.load(handle)\n",
    "        \n",
    "        with open(self.cfg.saved_model_path + '/image_features_extract_model.json', 'r') as json_file:\n",
    "            json_savedModel= json_file.read()\n",
    "        self.image_features_extract_model = tf.keras.models.model_from_json(json_savedModel)\n",
    "        self.image_features_extract_model.load_weights(self.cfg.saved_model_path + '/image_features_extract_model.h5')\n",
    "        vocab_size = cfg.max_tokenized_words + 1\n",
    "\n",
    "        s = tf.zeros([32, 64, 2048], tf.int32)\n",
    "\n",
    "        self.encoder=Encoder(self.cfg.embedding_dim)\n",
    "\n",
    "        self.decoder=Decoder(self.cfg.embedding_dim, self.cfg.units, vocab_size,cfg)\n",
    "\n",
    "        features = self.encoder(s)\n",
    "\n",
    "        hidden = self.decoder.init_state(batch_size=self.cfg.batch_size)\n",
    "        dec_input = tf.expand_dims([self.tokenizer.word_index['<start>']] * self.cfg.batch_size, 1)\n",
    "\n",
    "        predictions, hidden_out, attention_weights= self.decoder(dec_input, features, hidden)\n",
    "\n",
    "        self.decoder.load_weights(self.cfg.saved_model_path + \"/decoder.h5\")\n",
    "        self.encoder.load_weights(self.cfg.saved_model_path + \"/encoder.h5\")\n",
    "\n",
    "    def evaluate(self,image):\n",
    "        attention_features_shape = 64\n",
    "        attention_plot = np.zeros((self.cfg.MAX_SEQ_LENGTH, attention_features_shape))\n",
    "\n",
    "        hidden = self.decoder.init_state(batch_size=1)\n",
    "\n",
    "        # process the input image to desired format before extracting features\n",
    "        temp_input = tf.expand_dims(image, 0) \n",
    "        # Extract features using our feature extraction model\n",
    "        img_tensor_val = self.extract_image_features(temp_input)\n",
    "        # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "        # extract the features by passing the input to encoder\n",
    "        features = self.encoder(img_tensor_val)\n",
    "\n",
    "        dec_input = tf.expand_dims([self.tokenizer.word_index['<start>']], 0)\n",
    "        result = []\n",
    "\n",
    "        for i in range(self.cfg.MAX_SEQ_LENGTH):\n",
    "            # get the output from decoder\n",
    "            predictions, hidden, attention_weights = self.decoder(dec_input, features, hidden)\n",
    "\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "            #extract the predicted id(embedded value) which carries the max value\n",
    "            predicted_id = tf.argmax(tf.transpose(predictions))\n",
    "            predicted_id = predicted_id.numpy()[0]\n",
    "            # map the id to the word from tokenizer and append the value to the result list\n",
    "            result.append(self.tokenizer.index_word[predicted_id])\n",
    "\n",
    "            if self.tokenizer.index_word[predicted_id] == '<end>':\n",
    "                return result, attention_plot,predictions\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        attention_plot = attention_plot[:len(result), :]\n",
    "        return result, attention_plot,predictions\n",
    "\n",
    "    def process_image_file(self,msg,shape):\n",
    "        temp_file = '/tmp/temp.bin'\n",
    "        f = open(temp_file, \"wb\")\n",
    "        f.write(msg)\n",
    "        f.close()\n",
    "\n",
    "        image_tensor,label = Util.read_image(temp_file,[])\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        result, attention_plot,pred_test  = self.evaluate(image_tensor)\n",
    "        t1 = time.perf_counter() - t0\n",
    "        pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "        send_json_dict = {}\n",
    "        send_json_dict['response'] = 'OK'\n",
    "        send_json_dict['pred_caption'] = pred_caption\n",
    "        send_json_dict['tail_model_time'] = t1\n",
    "\n",
    "        app_json = json.dumps(send_json_dict)\n",
    "\n",
    "        return str(app_json)\n",
    "\n",
    "    def process_image_tensor(self,msg,shape):\n",
    "        generated_np_array = np.frombuffer(msg, dtype=float32)\n",
    "        generated_image_np_array = generated_np_array.reshape(shape)\n",
    "        image_tensor = tf.convert_to_tensor(generated_image_np_array, dtype=tf.float32)\n",
    "\n",
    "        result, attention_plot,pred_test  = self.evaluate(image_tensor)\n",
    "        pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "        return pred_caption\n",
    "        \n",
    "    def extract_image_features(self, sample_img_batch):\n",
    "        features = self.image_features_extract_model(sample_img_batch)\n",
    "        features = tf.reshape(features, [sample_img_batch.shape[0],8*8, 2048])\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 9] Bad file descriptor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1002d660ddb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtailModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtailModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_image_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-850247ac1eb4>\u001b[0m in \u001b[0;36maccept_connections\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ctrl+c,Exiting gracefully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py373/lib/python3.7/socket.py\u001b[0m in \u001b[0;36maccept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mFor\u001b[0m \u001b[0mIP\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maddress\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhostaddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;31m# Issue #7995: if no default timeout is set and the listening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/suphale/anaconda3/envs/py373/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/suphale/anaconda3/envs/py373/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-850247ac1eb4>\", line 48, in handle_client\n",
      "    obj = json.loads(received_data)\n",
      "  File \"/home/suphale/anaconda3/envs/py373/lib/python3.7/json/__init__.py\", line 348, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/home/suphale/anaconda3/envs/py373/lib/python3.7/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/home/suphale/anaconda3/envs/py373/lib/python3.7/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = Config(None)\n",
    "tailModel = TailModel(cfg)\n",
    "server = Server(cfg, tailModel)\n",
    "server.register_callback('data',tailModel.process_image_tensor)\n",
    "server.register_callback('file',tailModel.process_image_file)\n",
    "server.accept_connections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d8a323e6010706682c07af791323eacfc072764aa514c33420848fded080be"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('py373': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}